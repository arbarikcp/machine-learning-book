{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb3efd3",
   "metadata": {},
   "source": [
    "- When error/loss is too high, then plotting it becomes difficult.\n",
    "- We can use log10 for the losses to plot the graph.\n",
    "- **What log10 Does**:\n",
    "- log10 takes the logarithm base 10 of your loss values. This transforms the scale of your y-axis.\n",
    "#### Why Use log10 for Plotting Losses?\n",
    "- During the training typical loss looks like.\n",
    "- ```\n",
    "    losses = [100, 50, 25, 5, 2, 1.5, 1.2, 1.1, 1.05, 1.02, 1.01]\n",
    "\n",
    "    If you plot this directly:\n",
    "    100 |*\n",
    "    90 |\n",
    "    80 |\n",
    "    70 |\n",
    "    60 |\n",
    "    50 |*\n",
    "    40 |\n",
    "    30 |\n",
    "    20 |\n",
    "    10 |*\n",
    "    0 |_________________*_*_*_*_*_*_*_*\n",
    "        1 2 3 4 5 6 7 8 9 10 11\n",
    "  ```\n",
    "##### The problem:\n",
    "- Early drops (100→50→25) are huge and visible\n",
    "- Later improvements (1.05→1.02→1.01) are tiny and invisible at the bottom\n",
    "- But those small improvements are still important!  \n",
    "\n",
    "##### Solution With log10:\n",
    "- ```\n",
    "np.log10(losses) -> [2.0, 1.7, 1.4, 0.7, 0.3, 0.18, 0.08, 0.04, 0.02, 0.009, 0.004]\n",
    "\n",
    "\n",
    "Now when you plot:\n",
    "\n",
    "    2.0 |*\n",
    "    1.5 |  *\n",
    "    1.0 |    *\n",
    "    0.5 |       *\n",
    "    0.0 |         * * * * * * *\n",
    "        1 2 3 4 5 6 7 8 9 10 11\n",
    "    ```\n",
    "##### The benefits:\n",
    "\n",
    "- You can see both the large early drops AND the small later improvements\n",
    "- The entire learning curve is visible\n",
    "- Easier to detect if learning has plateaued\n",
    "\n",
    "\n",
    "- **Why This Matters for Your High Learning Rate:**\n",
    "- With learning rate = 0.1 (which is high):\n",
    "\n",
    "- **Early epochs**: Loss might drop dramatically (e.g., 1000 → 100)\n",
    "- **Middle epochs**: Loss decreases moderately (e.g., 100 → 10)\n",
    "- **Late epochs**: Loss fine-tunes slowly (e.g., 1.5 → 1.0)\n",
    "\n",
    "- **Without log10**, you'd only see the early dramatic drop and think \"did it even improve after epoch 2?\"\n",
    "- **With log10**, you can see all phases of learning and verify:\n",
    "\n",
    "  - ✓ Model is still improving\n",
    "  - ✓ Learning hasn't plateaued\n",
    "  - ✓ No oscillations or divergence\n",
    "- **Key Takeaway**:\n",
    "- log10 compresses large values and expands small values, making it perfect for visualizing exponentially decreasing quantities like loss.\n",
    "- Think of it like this:\n",
    "\n",
    "    - **Linear scale**: Good for showing absolute differences\n",
    "    - **Log scale**: Good for showing relative/percentage changes\n",
    "\n",
    "- **Since we care about \"is the model improving?\" rather than \"by exactly how much?\", log scale is ideal!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "la",
   "language": "python",
   "name": "la"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
