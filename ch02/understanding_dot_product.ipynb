{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2d6646",
   "metadata": {},
   "source": [
    "#### Understanding numpy array axis\n",
    "- **1D Array (1 axis)**\n",
    "    ```\n",
    "        arr = np.array([1, 2, 3, 4])\n",
    "        # Shape: (4,)\n",
    "        # axis 0 → goes along the elements\n",
    "    ```\n",
    "- **2D Array (2 axes)**\n",
    "  ```\n",
    "    A = np.array([[1, 2, 3],\n",
    "                [4, 5, 6]])\n",
    "    # Shape: (2, 3)\n",
    "    # axis 0 → goes DOWN (rows)\n",
    "    # axis 1 → goes ACROSS (columns)\n",
    "\n",
    "    #        ← axis 1 →\n",
    "    #      col0 col1 col2\n",
    "    #  ↓   [  1,   2,   3 ]  row 0\n",
    "    # axis [  4,   5,   6 ]  row 1\n",
    "    #  0\n",
    "  ```       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463ff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "10\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "oned_arr = np.array([1, 2, 3, 4])\n",
    "print(oned_arr.shape) # its shape is (4,) not (1,4), When shape is (4,) it means it is a vector, not a matrix.\n",
    "\n",
    "print(np.sum(oned_arr,axis=0)) # this will return a scalar , -> 10\n",
    "\n",
    "\n",
    "twod_arr = np.array([[1, 2, 3],\n",
    "                    [4, 5, 6]])\n",
    "print(twod_arr.shape)\n",
    "\n",
    "print(np.sum(twod_arr,axis=0)) # this will along axis 0 (rows),  return a 1D array, -> [5 7 9]\n",
    "print(np.sum(twod_arr,axis=1)) # this will along axis 1 (columns),  return a 1D array, -> [ 6 15]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c742f5",
   "metadata": {},
   "source": [
    "####  Understanding axis in 3d array\n",
    "```\n",
    "    B = np.array([[[1, 2, 3, 4],\n",
    "                [3, 4, 5, 6]],\n",
    "                \n",
    "                [[5, 6,7,8],\n",
    "                [7, 8,9,2]],\n",
    "\n",
    "                [[5, 3, 4, 2],\n",
    "                [4, 5, 8, 6]]])\n",
    "    # Shape: (3, 2, 4)\n",
    "    # axis 0 → depth (which \"layer\")\n",
    "    # axis 1 → rows (going down)\n",
    "    # axis 2 → columns (going across)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dce536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 4)\n",
      "[[11 11 14 14]\n",
      " [14 17 22 14]]\n",
      "[[ 4  6  8 10]\n",
      " [12 14 16 10]\n",
      " [ 9  8 12  8]]\n",
      "[[10 18]\n",
      " [26 26]\n",
      " [14 23]]\n"
     ]
    }
   ],
   "source": [
    "B = np.array([[[1, 2, 3, 4],\n",
    "                [3, 4, 5, 6]],\n",
    "                \n",
    "                [[5, 6,7,8],\n",
    "                [7, 8,9,2]],\n",
    "\n",
    "                [[5, 3, 4, 2],\n",
    "                [4, 5, 8, 6]]])\n",
    "print(B.shape)\n",
    "\n",
    "print(np.sum(B,axis=0)) # this will, sum along axis 0 (depth), return a 2D array, -> [[11 11 14 14] [14 17 22 14]]\n",
    "print(np.sum(B,axis=1)) # this will, sum along axis 1 (rows), return a 2D array, -> [[ 4 6 8 10] [12 14 16 10] [9 8 12 8]]\n",
    "print(np.sum(B,axis=2)) # this will, sum along axis 2 (columns), return a 2D array, -> [[10 18 ],[26 26] [14,23]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ff929",
   "metadata": {},
   "source": [
    "##### Quick recap on Axis\n",
    "\n",
    "`arr = np.random.rand(3, 4, 5, 6)  # 4D array`\n",
    "\n",
    "#### Shape tells you axes:\n",
    "#### (3, 4, 5, 6)\n",
    "####  ↑  ↑  ↑  ↑\n",
    "####  │  │  │  └─ axis 3 (size 6)\n",
    "####  │  │  └──── axis 2 (size 5)\n",
    "####  │  └─────── axis 1 (size 4)\n",
    "####  └────────── axis 0 (size 3)\n",
    "\n",
    "#### axis = 0 → operate along first dimension\n",
    "#### axis = -1 → operate along last dimension (same as axis=3 here)\n",
    "#### axis = -2 → second-to-last (same as axis=2 here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745065d",
   "metadata": {},
   "source": [
    "### Dot Product\n",
    "\n",
    "##### .dot() .matmul() , @ \n",
    "- If both `a` and `b` are 1-D arrays, it is inner product of vectors, remember this is a vector dot product.\n",
    "    - np.dot(np.array([1, 2, 3]), np.array([4, 5, 6]))  ->        # 1*4 + 2*5 + 3*6 = 32\n",
    "- If both `a` and `b` are 2-D arrays, it is matrix multiplication,\n",
    "- If either `a` or `b` is 0-D (scalar), it is equivalent to a scalar multiplication to the vector.\n",
    "- If `a` is an N-D array and `b` is a 1-D array, it is a sum product over\n",
    "      the last axis of `a` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "p = np.dot(a, b)        # 1*4 + 2*5 + 3*6 = 32\n",
    "print(p)\n",
    "print( a @ b)\n",
    "print(np.matmul(a, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e3925b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12 15 18]\n",
      " [26 33 40]\n",
      " [40 51 62]]\n",
      "[[12 15 18]\n",
      " [26 33 40]\n",
      " [40 51 62]]\n",
      "[[12 15 18]\n",
      " [26 33 40]\n",
      " [40 51 62]]\n"
     ]
    }
   ],
   "source": [
    "# A -> 3 x 2\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4],\n",
    "              [5, 6]])\n",
    "# B -> 2 x 3\n",
    "B = np.array([[2,3,4],\n",
    "              [5,6,7]])\n",
    "# C -> 3 x 3\n",
    "C = np.dot(A, B)\n",
    "print(C)\n",
    "C = np.matmul(A, B)\n",
    "print(C)\n",
    "C = A @ B\n",
    "print(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c55f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4]\n",
      " [ 6  8]\n",
      " [10 12]]\n"
     ]
    }
   ],
   "source": [
    "# A -> 3 x 2\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4],\n",
    "              [5, 6]])\n",
    "# B -> scalar\n",
    "B = 2\n",
    "C = np.dot(A, B)\n",
    "print(C)\n",
    "# C = np.matmul(A, B) -> this will throw error as the dimensions are not compatible\n",
    "# C = A @ B -> this will throw error as the dimensions are not compatible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdae428a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([1,2])\n",
    "B = 2\n",
    "C = np.dot(A, B)\n",
    "print(C)\n",
    "# C = np.matmul(A, B) -> this will throw error as the dimensions are not compatible\n",
    "# C = A @ B -> this will throw error as the dimensions are not compatible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189bda06",
   "metadata": {},
   "source": [
    "##### If `a` is an N-D array and `b` is a 1-D array, it is a sum product over the last axis of `a` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a69e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2],\n",
    "              [3, 4],\n",
    "              [5, 6]])  # shape (3, 2)\n",
    "\n",
    "B = np.array([2, 3])    # shape (2,)\n",
    "\n",
    "# \"Last axis of A\" = the columns (axis 1), which has size 2 , along last axis of A, we have 3 vectors [1,2] [3,4] [5,6]\n",
    "# Now we have to multiply each of these vector with B vector (2,)\n",
    "# result[0] = 1*2 + 2*3 = 8   # first row of A · B\n",
    "# result[1] = 3*2 + 4*3 = 18  # second row of A · B  \n",
    "# result[2] = 5*2 + 6*3 = 28  # third row of A · B\n",
    "\n",
    "# result = [8, 18, 28]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e3d1e",
   "metadata": {},
   "source": [
    "### Intutions\n",
    "\n",
    "#### net_input calculation\n",
    "- We have a matrix in shape (n_samples, n_features) -> X\n",
    "- We initialized weights for each feature, So our W shape is (n_features,) -> W\n",
    "- For net input, we need to calculate the net input for each sample. \n",
    "  - for example X[0] is one sample, it is of shape (n_features,) \n",
    "  - Now this X[0] must multiply with W -> np.dot(X[0], W) ->  this will be one scalar.\n",
    "    - Like this we need to calculate for X[1], X[2] ... X[n] -> each one is one one sample, We need to find the net_input for each sample.\n",
    "    - np.dot(X, W) where X is (n_samples, n_features) and w is (n_features,) → (n_samples,) , its not (n_samples,1) -> numpy reduces the dimenson to only (n_samples,)\n",
    "- Mathematically this is xᵀw, but NumPy doesn’t need you to write x.T because 1D arrays have no row/column orientation. \n",
    "\n",
    "#### gradiant update\n",
    "- We found our net_input for each sample, it is of shape (n_sample,)\n",
    "- Now our errors is `errors = (y - net_input)`\n",
    "- errors is also shape (n_sample,) -> each sample has an error.\n",
    "- Our goal is to find the gradiant for **each feature**. So that each feature will be updated by its own gradiant. So we need gradiant in the shape of (n_features,)\n",
    "- X is in shape (n_samples,n_features)\n",
    "- error is in shape (n_sample), one error per sample.\n",
    "- **Now lets see how each feature and weight affected the error**\n",
    "  - In other word **How correlated is this feature with the errors?**\n",
    "  - Or more specifically **\"When this feature is large, do the errors tend to be large? When it's small, are errors small?\"**\n",
    "- Suppose we have 3 features, feature_0. And we have 3 samples.\n",
    "  -  `feature_0 = [1, 4, 7]   # Feature 0 across all samples`\n",
    "  -  `errors = [0.5, -0.3, 0.2] # Errors for all samples` \n",
    "  -  `feature_0 · errors = 1*0.5 + 4*(-0.3) + 7*0.2  # Dot product:`\n",
    "  -  **Let's analyze each multiplication:**\n",
    "     - **Sample 1**: `1 * 0.5 = 0.5`\n",
    "       - Feature value: 1\n",
    "       - Error: +0.5 (predicted too low) # remember error is (actual target - predicted target).\n",
    "       - **Interpretation**: \"In sample 1, feature 0 had value 1, and we under-predicted. Since the feature value was positive, maybe we should INCREASE weight 0 to predict higher next time.\" \n",
    "     - **Sample 2**: 4 * (-0.3) = -1.2 \n",
    "       - Feature value: 4\n",
    "       - Error: -0.3 (predicted too high)\n",
    "       - **Interpretation**: \"In sample 2, feature 0 had value 4 (larger!), and we over-predicted. Since the feature value was positive AND large, we should DECREASE weight 0 significantly to predict lower next time.\"\n",
    "    - **Sample 3**: 7 * 0.2 = 1.4\n",
    "      - Feature value: 7\n",
    "      - Error: +0.2 (predicted too low)\n",
    "      - **Interpretation**: \"In sample 3, feature 0 had value 7 (even larger!), and we under-predicted. Since the feature value was large and positive, we should INCREASE weight 0 a lot.\"\n",
    "    - **Overall**: Sample 1 is saying increase weight for feature 0. Sample 2 is saying decrease the weight for feature 0, sample 3 is saying increase the weight for feature 0.   # W[0] is the weight for feature 0.\n",
    "      - **The Sum (0.5 - 1.2 + 1.4 = 0.7):**\n",
    "        - This total tells you the net direction and strength of the update:\n",
    "        - **Positive result** (+0.7): \"Overall, across all samples, increasing this weight will reduce errors\"\n",
    "        - **Negative result**: \"Overall, decreasing this weight will reduce errors\"\n",
    "        - **Large magnitude**: \"This feature has strong correlation with errors - adjust it a lot!\"\n",
    "        - **Small magnitude**: \"This feature doesn't correlate much with errors - adjust it a little\"\n",
    "      - **Notice**: this sum will be very large or small based the number of samples, if we have 100K samples then sum of all feature_0 * error will be very large. So we need to take the mean. \n",
    "- As we have \n",
    "  - X is in shape (n_samples,n_features)\n",
    "  - error is in shape (n_sample), one error per sample.\n",
    "  - To get the gradiant of for each weight, we need to multiply each feature vector (i.e feature_0 across all sample) to  error vector (we have one error per sample.)\n",
    "  - Xᵀ will be of shape (n_features, n_samples) , each row is one feature value across sample.\n",
    "  - Xᵀ.dot(error) -> each feature row vector will be multiplied by error vector -> result will be of shape (n_features,). Each one indicates a +ve or -ve value. based on this its weight will be adjusted.\n",
    "  - Take the mean **Xᵀ.dot(error)/ X.shape[0]** #  X.shape[0] means number of rows, means number of samples.\n",
    "  - We should not use this value directly to update our weight. As this value only indicates direction and magnitude. We should multiply this with our learning rate. **self.eta * 2.0 * X.T.dot(errors) / X.shape[0]** -> this gives a smaller number which need to be added to w.\n",
    "  - \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "la",
   "language": "python",
   "name": "la"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
