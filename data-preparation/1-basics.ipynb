{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a74157c8",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Basics\n",
    "- The philosophy of data preparation is to discover how to best expose the unknown underlying structure of the problem to the learning algorithms.\n",
    "- This often requires an iterative path of experimentation through a suite of different data preparation techniques in order to discover what works well or best.\n",
    "- Most of the machine learning algorithms are well understood and they are just routines. with amazing fully featured open-source machine learning libraries like scikit-learn in Python. **The thing that is different from project to project is the data.**\n",
    "- **The challenge of data preparation** is that each dataset is unique and different. Datasets differ in the number of variables (tens, hundreds, thousands, or more), the types of the variables (numeric, nominal, ordinal, boolean), the scale of the variables, the drift in the values over time, and more\n",
    "\n",
    "#### Applied Machine Learning Process\n",
    "- You may be the first person (ever!) to work on the specific predictive modeling problem. That does not mean that others have not worked on similar prediction tasks or perhaps even the same high-level task, but you may be the first to use the specific data that you have collected.\n",
    "-  No one can tell you what the best results are or might be, or what algorithms to use to achieve them. \n",
    "- You must establish a baseline in performance as a point of reference to compare all of your models and you must discover what algorithm works best for your specific dataset.\n",
    "- This processis sometimes referred to as the applied **machine learning process**, **data science process**, or the older name **knowledge discovery in databases** (KDD).\n",
    "- The process using the few high-level steps:\n",
    "\n",
    "    - Step 1: Define Problem\n",
    "    - Step 2: Data Collection\n",
    "    - Step 3: Data Preparation\n",
    "    - Step 4: Model Building    \n",
    "    - Step 5: Model Evaluation\n",
    "    - Step 6: Finalize the model\n",
    "\n",
    "#### Define the problem\n",
    "- The step before data preparation involves defining the problem. As part of defining the problem, this may involve many sub-tasks.\n",
    "\n",
    "    - Gather data from the problem domain.\n",
    "    - Discuss the project with subject matter experts.\n",
    "    - Select those variables to be used as inputs and outputs for a predictive model. \n",
    "    - Review the data that has been collected.\n",
    "    - Summarize the collected data using statistical methods.\n",
    "    - Visualize the collected data using plots and charts.\n",
    "\n",
    "#### Data Preparation\n",
    "- Raw data typically cannot be used directly. This is because of reasons\n",
    "    - Machine learning algorithms require data to be numerics.\n",
    "    - Some machine learning algorithms impose requirements on the data.\n",
    "    - Statistical noise and errors in the data may need to be corrected.\n",
    "    - Complex nonlinear relationships may be teased out of the data.\n",
    "- There are common or standard tasks that you may use or explore during the data preparation\n",
    "    - **Data Cleaning**: Identifying and correcting mistakes or errors in the data.\n",
    "    - **Feature Selection**: Identifying those input variables that are most relevant to the task.\n",
    "    - **Data Transforms**: Changing the scale or distribution of variables.\n",
    "    - **Feature Engineering**: Deriving new variables from available data.\n",
    "    - **Dimensionality Reduction**: Creating compact projections of the data.\n",
    "\n",
    "### What Is Data in Machine Learning\n",
    "- Predictive modeling projects involve learning from data. Data refers to examples or cases from the domain that characterize the problem you want to solve. \n",
    "- In supervised learning, data is composed of examples where each example has an input element that will be provided to a model and an output or target element that the model is expected to predict.\n",
    "- **Classification** is an example of a supervised learning problem where the target is a label, \n",
    "- and **Regression** is an example of a supervised learning problem where the target is a number.\n",
    "\n",
    "- Most common type of input data is typically referred to as tabular data or structured data.\n",
    "    - **Row**. A single example from the domain, often called an instance, example or sample in machine learning.\n",
    "    - **Column**. A single property recorded for each example, often called a variable, **predictor**, or feature in machine learning.\n",
    "    - **Input Variables**: Columns in the dataset provided to a model in order to make a prediction.\n",
    "    - **Output Variable**: Column in the dataset to be predicted by a model.\n",
    "\n",
    "- When you collect your data, you may have to transform it so it forms one large table. For example, if you have your data in a relational database, it is common to represent entities in separate tables in what is referred to as a normal form so that redundancy is minimized.\n",
    "- In order to create one large table with one row per subject or entity that you want to model, you may need to reverse this process and introduce redundancy in the data in a process referred to as denormalization.\n",
    "\n",
    "- **Raw data**: Data in the form provided from the domain.\n",
    "- A **feature** is a numeric representation of an aspect of raw data. Features sit between data and models in the machine learning pipeline.\n",
    "- **Feature engineering** is the act of extracting features from raw data and transforming them into formats that are suitable for the machine learning model.\n",
    "\n",
    "- Data preparation can make or break a model’s predictive ability. Different models have different sensitivities to the type of predictors in the model; how the predictors enter the model is also important.\n",
    " - **For example**: \n",
    "    - Linear machine learning models that expect each numeric input variable to have a Gaussian probability distribution.\n",
    "    - This means that if you have input variables that are not Gaussian or nearly Gaussian, you might need to change them so that they are Gaussian or more Gaussian\n",
    "    - Some algorithms are known to perform worse if there are input variables that are irrelevant or redundant to the target variable. There are also algorithms that are negatively impacted if two or more input variables are highly correlated.\n",
    "    - There are also algorithms that have very few requirements about the probability distribution of input variables or the presence of redundancies, but in turn, may require many more examples (rows) in order to learn how to make good predictions.\n",
    "\n",
    "- The idea that there are different ways to represent predictors in a model, and that some of these representations are better than others, leads to the idea of **feature engineering**  . \n",
    "- The performance of a machine learning algorithm is only as good as the data used to train it. This is often summarized as **garbage in, garbage out**.\n",
    "\n",
    "- A dataset may be a weak representation of the problem we are trying to solve for many reasons.\n",
    "    - **Complex Data**: Raw data contains compressed complex nonlinear relationships that may need to be exposed\n",
    "    - **Messy Data**: Raw data contains statistical noise, errors, missing values, and conflicting examples.\n",
    "\n",
    "- We can think about getting the most out of our predictive modeling project in two ways:\n",
    "    - **focus on the model**:\n",
    "        - We could minimally prepare the raw data and begin modeling. This puts full onus on the model to tease out the relationships in the data and learn the mapping function from inputs to outputs as best it can.\n",
    "        - May require a large dataset and a flexible and powerful machine learning algorithm with few expectations, such as random forest or gradient boosting.\n",
    "    - **focus on the data**:\n",
    "        - Alternately, we could push the onus back onto the data and the data preparation process. This requires that each row of data best expresses the information content of the data for modeling\n",
    "\n",
    "- **Although the algorithms are well understood operationally, most don’t have satisfiable theories about why they work or how to map algorithms to problems.This is why each predictive modeling project is empirical rather than theoretical, requiring a process of systematic experimentation of algorithms on data.**\n",
    "\n",
    "#### Data Cleaning\n",
    "- Data cleaning involves fixing systematic problems or errors in messy data.\n",
    "- The most useful data cleaning involves deep domain expertise and could involve identifying and addressing specific observations that may be incorrect.\n",
    "- There are many reasons data may have incorrect values, such as being mistyped, corrupted, duplicated, and so on.\n",
    "\n",
    "- **Techniques:**:\n",
    "    - **Basics**\n",
    "        - Redundant Samples\n",
    "        - Redundant Features\n",
    "    - **Outliers**\n",
    "        - Extreme Values\n",
    "    - **Missing**\n",
    "        - Mark\n",
    "        - Impute\n",
    "\n",
    "#### Feature selection\n",
    "- ![Feature selection](./images/feature.jpeg)\n",
    "\n",
    "#### Data Transforms\n",
    "- **Data Types**\n",
    "    - **Numeric Data Type**\n",
    "        - Integer\n",
    "        - Float\n",
    "    - **Categorical Data Type**\n",
    "        - **Nominal**: Labels with no rank ordering. \n",
    "        - **Ordinal**: Labels with a rank ordering.\n",
    "        - **Boolean**: Values True and False.\n",
    "\n",
    "- **Discretization Transform**: Encode a numeric variable as an ordinal variable .\n",
    "- **Ordinal Transform**: Encode a categorical variable into an integer variable .\n",
    "- **One Hot Transform**: Encode a categorical variable into binary variables .\n",
    "- **Normalization Transform**: Scale a variable to the range 0 and 1.\n",
    "- **Standardization Transform**: Scale a variable to a standard Gaussian\n",
    "- **Power Transform**: Change the distribution of a variable to be more Gaussian.\n",
    "- **Quantile Transform**: Impose a probability distribution such as uniform or Gaussian.\n",
    "- **Polynomial Transform**: Create copies of numerical input variables that are raised to a power.\n",
    "\n",
    "- ![Data Transforms](./images/data-transform.jpeg)\n",
    "\n",
    "##### Dimensionality Reduction\n",
    "- The number of input features for a dataset may be considered the dimensionality of the data.\n",
    "- For example, two input variables together can define a two-dimensional area where each row of data defines a point in that space. This idea can then be scaled to any number of input variables to create large multi-dimensional hyper-volumes.\n",
    "- The problem is, the more dimensions this space has (e.g. the more input variables), the more likely it is that the dataset represents a very sparse and likely unrepresentative sampling of that space. This is a common issue in machine learning known as the **curse of dimensionality**.\n",
    "- **Dimensionality reduction** is the process of reducing the number of input variables in a dataset.\n",
    "\n",
    "- ![Dimensionality Reduction](./images/Dimenson_reduction.jpeg)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
