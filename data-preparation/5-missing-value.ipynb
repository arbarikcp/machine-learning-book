{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18f9167",
   "metadata": {},
   "source": [
    "#### Missing data\n",
    "\n",
    "- Real-world data often has missing values. Data can have missing values for a number of reasons such as observations that were not recorded and data corruption. \n",
    "- Handling missing data is important as many machine learning algorithms do not support data with missing values.\n",
    "- Its important to understand the missing value. Sometime a zero value or a null value can be a valid value and not missing. We need to work with domian experts to identify the missing value, and set the baseline.\n",
    "- Sometime missing values are indicated by out of range value like -1. or \"?\" or empty string , or undefined, or NA, or NaN, or NULL, or None, or etc. it can be anything. **We need to work with domian experts to identify the missing value, and set the baseline.**\n",
    "- We can see that columns 1, 2 and 5 have just a few zero values, whereas columns 3 and 4 show a lot more, nearly half of the rows. This highlights that different missing value strategies may be needed for different columns\n",
    "\n",
    "- **When a predictor is discrete in nature, missingness can be directly encoded into the predictor as if it were a naturally occurring category.**\n",
    "    - If a feature (column) is categorical/discrete — meaning it contains categories, then instead of removing rows or imputing values, you can treat missing values as a new valid category.\n",
    "    - Suppose we have a column having 3 caegories like \"Red, Blue, Green\" and we have a missing value. We can encode it as \"Missing\". So our total categories will be \"Red, Blue, Green, Missing\".\n",
    "\n",
    "    - **Why does this make sense?**\n",
    "        - Because in categorical data:\n",
    "            - Missing may not be random.\n",
    "            - Missingness itself could contain meaning.\n",
    "        - **example**:\n",
    "            | Job Type      | Approved Loan? |\n",
    "            | ------------- | -------------- |\n",
    "            | Salaried      | Yes            |\n",
    "            | Self-employed | Yes            |\n",
    "            | NaN           | No             |\n",
    "            | Salaried      | Yes            |\n",
    "\n",
    "        - People with missing job type might represent:\n",
    "            - Low documentation applicants\n",
    "            - Risky profiles\n",
    "            - Deliberate non-disclosure\n",
    "\n",
    "        - So treating missingness as a signal lets the model learn patterns from it.  \n",
    "\n",
    "    - **When does this NOT apply?**\n",
    "        - If a feature is continuous, then missingness is not a valid category. \n",
    "        - example : these below col-values doesn't make sense.\n",
    "            - Height = \"Missing\"\n",
    "            - Salary = -99999 (bad practice) \n",
    "        - Instead, we typically:\n",
    "            - Impute with mean/median\n",
    "            - Use special encodings like indicator flags for missingness\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b5db6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>148</th>\n",
       "      <th>72</th>\n",
       "      <th>35</th>\n",
       "      <th>0</th>\n",
       "      <th>33.6</th>\n",
       "      <th>0.627</th>\n",
       "      <th>50</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>767.000000</td>\n",
       "      <td>767.000000</td>\n",
       "      <td>767.000000</td>\n",
       "      <td>767.000000</td>\n",
       "      <td>767.000000</td>\n",
       "      <td>767.000000</td>\n",
       "      <td>767.000000</td>\n",
       "      <td>767.000000</td>\n",
       "      <td>767.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.842243</td>\n",
       "      <td>120.859192</td>\n",
       "      <td>69.101695</td>\n",
       "      <td>20.517601</td>\n",
       "      <td>79.903520</td>\n",
       "      <td>31.990482</td>\n",
       "      <td>0.471674</td>\n",
       "      <td>33.219035</td>\n",
       "      <td>0.348110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.370877</td>\n",
       "      <td>31.978468</td>\n",
       "      <td>19.368155</td>\n",
       "      <td>15.954059</td>\n",
       "      <td>115.283105</td>\n",
       "      <td>7.889091</td>\n",
       "      <td>0.331497</td>\n",
       "      <td>11.752296</td>\n",
       "      <td>0.476682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.500000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                6         148          72          35           0        33.6  \\\n",
       "count  767.000000  767.000000  767.000000  767.000000  767.000000  767.000000   \n",
       "mean     3.842243  120.859192   69.101695   20.517601   79.903520   31.990482   \n",
       "std      3.370877   31.978468   19.368155   15.954059  115.283105    7.889091   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
       "50%      3.000000  117.000000   72.000000   23.000000   32.000000   32.000000   \n",
       "75%      6.000000  140.000000   80.000000   32.000000  127.500000   36.600000   \n",
       "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
       "\n",
       "            0.627          50           1  \n",
       "count  767.000000  767.000000  767.000000  \n",
       "mean     0.471674   33.219035    0.348110  \n",
       "std      0.331497   11.752296    0.476682  \n",
       "min      0.078000   21.000000    0.000000  \n",
       "25%      0.243500   24.000000    0.000000  \n",
       "50%      0.371000   29.000000    0.000000  \n",
       "75%      0.625000   41.000000    1.000000  \n",
       "max      2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "dataset = read_csv('./data/pima-indians-diabetes.csv')\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec2456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Lets take 0 as a missing value\n",
    "# Then lets find out all the missing values.\n",
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "dataset = read_csv('./data/pima-indians-diabetes.csv', header=None) # count the number of missing values for each column \n",
    "num_missing = (dataset[[1,2,3,4,5]] == 0).sum()\n",
    "# report the results\n",
    "print(num_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e577e500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "6      0\n",
      "7      0\n",
      "8      0\n",
      "dtype: int64\n",
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>110.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>168.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>139.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>189.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>166.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>107.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>103.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>115.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1     2     3      4     5      6   7  8\n",
       "0    6  148.0  72.0  35.0    NaN  33.6  0.627  50  1\n",
       "1    1   85.0  66.0  29.0    NaN  26.6  0.351  31  0\n",
       "2    8  183.0  64.0   NaN    NaN  23.3  0.672  32  1\n",
       "3    1   89.0  66.0  23.0   94.0  28.1  0.167  21  0\n",
       "4    0  137.0  40.0  35.0  168.0  43.1  2.288  33  1\n",
       "5    5  116.0  74.0   NaN    NaN  25.6  0.201  30  0\n",
       "6    3   78.0  50.0  32.0   88.0  31.0  0.248  26  1\n",
       "7   10  115.0   NaN   NaN    NaN  35.3  0.134  29  0\n",
       "8    2  197.0  70.0  45.0  543.0  30.5  0.158  53  1\n",
       "9    8  125.0  96.0   NaN    NaN   NaN  0.232  54  1\n",
       "10   4  110.0  92.0   NaN    NaN  37.6  0.191  30  0\n",
       "11  10  168.0  74.0   NaN    NaN  38.0  0.537  34  1\n",
       "12  10  139.0  80.0   NaN    NaN  27.1  1.441  57  0\n",
       "13   1  189.0  60.0  23.0  846.0  30.1  0.398  59  1\n",
       "14   5  166.0  72.0  19.0  175.0  25.8  0.587  51  1\n",
       "15   7  100.0   NaN   NaN    NaN  30.0  0.484  32  1\n",
       "16   0  118.0  84.0  47.0  230.0  45.8  0.551  31  1\n",
       "17   7  107.0  74.0   NaN    NaN  29.6  0.254  31  1\n",
       "18   1  103.0  30.0  38.0   83.0  43.3  0.183  33  0\n",
       "19   1  115.0  70.0  30.0   96.0  34.6  0.529  32  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of marking missing values with nan values\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv('./data/pima-indians-diabetes.csv', header=None)\n",
    "\n",
    "# replace '0' values with 'nan'\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan)\n",
    "\n",
    "# count the number of nan values in each column\n",
    "print(dataset.isnull().sum())\n",
    "print(dataset.shape)\n",
    "dataset.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccfa5e3",
   "metadata": {},
   "source": [
    "##### There are ML algorithms which doesn't work on data with missing values.\n",
    "- for example LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9528ed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/lap/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "3 fits failed out of a total of 3.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/lap/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/miniconda3/envs/lap/lib/python3.9/site-packages/sklearn/discriminant_analysis.py\", line 544, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"/opt/miniconda3/envs/lap/lib/python3.9/site-packages/sklearn/base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/opt/miniconda3/envs/lap/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 964, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"/opt/miniconda3/envs/lap/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/opt/miniconda3/envs/lap/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "values = dataset.values\n",
    "X = values[:,0:8]\n",
    "y = values[:,8]\n",
    "\n",
    "# define the model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# define the model evaluation procedure\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# evaluate the model\n",
    "result = cross_val_score(model, X, y, cv=cv, scoring='accuracy') # report the mean performance\n",
    "print('Accuracy: %.3f' % result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d298d39",
   "metadata": {},
   "source": [
    "### Techniques to handle missing values\n",
    "\n",
    "#### 1. Removing/dropping rows with missing values\n",
    "- we can use panda's dropna() function to remove rows with missing values, any one column has missing values will be dropped.\n",
    "    - replace all missing values with a null/nan value.\n",
    "    - dropna() function can be used to remove rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67b5343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "(392, 9)\n"
     ]
    }
   ],
   "source": [
    "# summarize the shape of the raw data\n",
    "print(dataset.shape)\n",
    "# replace '0' values with 'nan'\n",
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, nan) # drop rows with missing values\n",
    "dataset.dropna(inplace=True)\n",
    "# summarize the shape of the data with missing rows removed \n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f76aa",
   "metadata": {},
   "source": [
    "#### 2. Statistical Imputation\n",
    "\n",
    "- It is good practice to identify and replace missing values for each column in your input data prior to modeling your prediction task. This is called **missing data imputation**.\n",
    "- A popular approach for data imputation is to calculate a statistical value for each column (such as a mean) and replace all missing values for that column with the statistic.\n",
    "\n",
    "- Common statistics calculated include:\n",
    "    - The column **mean** value.\n",
    "    - The column **median** value. \n",
    "    - The column **mode** value.\n",
    "    - A **constant** value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7a83e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>530101</td>\n",
       "      <td>38.50</td>\n",
       "      <td>66</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>45.00</td>\n",
       "      <td>8.40</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>534817</td>\n",
       "      <td>39.2</td>\n",
       "      <td>88</td>\n",
       "      <td>20</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>530334</td>\n",
       "      <td>38.30</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>33.00</td>\n",
       "      <td>6.70</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5290409</td>\n",
       "      <td>39.10</td>\n",
       "      <td>164</td>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>48.00</td>\n",
       "      <td>7.20</td>\n",
       "      <td>3</td>\n",
       "      <td>5.30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>530255</td>\n",
       "      <td>37.30</td>\n",
       "      <td>104</td>\n",
       "      <td>35</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>74.00</td>\n",
       "      <td>7.40</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0   1        2      3    4   5  6  7  8  9   ...     18    19 20    21 22  \\\n",
       "0  2   1   530101  38.50   66  28  3  3  ?  2  ...  45.00  8.40  ?     ?  2   \n",
       "1  1   1   534817   39.2   88  20  ?  ?  4  1  ...     50    85  2     2  3   \n",
       "2  2   1   530334  38.30   40  24  1  1  3  1  ...  33.00  6.70  ?     ?  1   \n",
       "3  1   9  5290409  39.10  164  84  4  1  6  2  ...  48.00  7.20  3  5.30  2   \n",
       "4  2   1   530255  37.30  104  35  ?  ?  6  2  ...  74.00  7.40  ?     ?  2   \n",
       "\n",
       "  23     24 25 26 27  \n",
       "0  2  11300  0  0  2  \n",
       "1  2   2208  0  0  2  \n",
       "2  2      0  0  0  1  \n",
       "3  1   2208  0  0  1  \n",
       "4  2   4300  0  0  2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = read_csv('./data/horse-colic.csv', header=None)\n",
    "dataframe.head()\n",
    "# we can see that this dataset has many \"?\" values, which are missing values.  So to let pandas handle missing values, we need to replace \"?\" with np.nan\n",
    "\n",
    "#dataframe.replace('?', np.nan, inplace=True)\n",
    "#dataframe.isnull().sum()\n",
    "#dataframe.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd64c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While reading the dataframe, we can mark which column vaue represents missing value\n",
    "dataframe = read_csv('./data/horse-colic.csv', header=None, na_values=['?'])\n",
    "dataframe.isnull().sum()\n",
    "\n",
    "# summarize the number of rows with missing values for each column\n",
    "for i in range(dataframe.shape[1]):\n",
    "    # count number of rows with missing values\n",
    "    n_miss = dataframe[[i]].isnull().sum()\n",
    "    perc = n_miss / dataframe.shape[0] * 100\n",
    "    print('> %d, Missing: %d (%.1f%%)' % (i, n_miss, perc)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ffc98",
   "metadata": {},
   "source": [
    "##### 2.1 Simple Imputation using sklearn.SimpleImputer\n",
    "- in SimpleImputer we can pass the Startegies like `strategy='mean'`, `strategy='median'`, `strategy='most_frequent'`, `strategy='constant'`\n",
    "\n",
    "- While imputing data, make sure that we don't do a data leakage. i.e. we should not use the test data to impute the train data. SO we should use a pipeline to impute the data and evaluate the model on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3607979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 1605\n",
      "After imputation Missing: 0\n"
     ]
    }
   ],
   "source": [
    "# statistical imputation transform for the horse colic dataset\n",
    "from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv('./data/horse-colic.csv', header=None, na_values='?')\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# summarize total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "\n",
    "# define imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X)\n",
    "\n",
    "# summarize total missing\n",
    "print('After imputation Missing: %d' % sum(isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "671d6dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.871 (0.051)\n"
     ]
    }
   ],
   "source": [
    "# evaluate mean imputation and random forest for the horse colic dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv('./data/horse-colic.csv', header=None, na_values='?')\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# define modeling pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8eb4c7",
   "metadata": {},
   "source": [
    "**2.1.1: Evaluate different imputation strtegies with our model**\n",
    "- we have different imputation strategies like `strategy='mean'`, `strategy='median'`, `strategy='most_frequent'`, `strategy='constant'`\n",
    "- We can try out all the startegies and see which one is best for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "264e1b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">mean 0.867 (0.049)\n",
      ">median 0.878 (0.062)\n",
      ">constant 0.880 (0.056)\n",
      "[array([0.86666667, 0.83333333, 0.86666667, 0.83333333, 0.93333333,\n",
      "       0.9       , 0.9       , 0.93333333, 0.76666667, 0.8       ,\n",
      "       0.9       , 0.86666667, 0.9       , 0.9       , 0.86666667,\n",
      "       0.83333333, 0.86666667, 0.8       , 0.8       , 0.93333333,\n",
      "       0.8       , 0.93333333, 0.83333333, 0.93333333, 0.86666667,\n",
      "       0.8       , 0.86666667, 0.93333333, 0.83333333, 0.9       ]), array([0.93333333, 0.8       , 0.9       , 0.9       , 0.93333333,\n",
      "       0.9       , 0.9       , 0.96666667, 0.73333333, 0.8       ,\n",
      "       0.86666667, 0.93333333, 0.9       , 0.9       , 0.86666667,\n",
      "       0.83333333, 0.9       , 0.8       , 0.8       , 0.93333333,\n",
      "       0.8       , 0.93333333, 0.83333333, 0.96666667, 0.86666667,\n",
      "       0.76666667, 0.9       , 0.96666667, 0.86666667, 0.93333333]), array([0.9       , 0.86666667, 0.86666667, 0.83333333, 0.93333333,\n",
      "       0.93333333, 0.9       , 0.96666667, 0.73333333, 0.8       ,\n",
      "       0.93333333, 0.9       , 0.8       , 0.9       , 0.86666667,\n",
      "       0.83333333, 0.9       , 0.83333333, 0.86666667, 0.93333333,\n",
      "       0.9       , 0.93333333, 0.86666667, 0.96666667, 0.86666667,\n",
      "       0.76666667, 0.9       , 0.93333333, 0.83333333, 0.93333333])]\n",
      "['mean', 'median', 'constant']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare statistical imputation strategies for the horse colic dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv('./data/horse-colic.csv', header=None, na_values='?')\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# evaluate each strategy on the dataset\n",
    "results = list()\n",
    "strategies = ['mean', 'median',  'constant']\n",
    "for s in strategies:\n",
    "\n",
    "\t# create the modeling pipeline\n",
    "\tpipeline = Pipeline(steps=[('i', SimpleImputer(strategy=s)), ('m', RandomForestClassifier())])\n",
    "\n",
    "\t# evaluate the model\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\tscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) # return list of scores for each fold\n",
    "\n",
    "\t# store results\n",
    "\tresults.append(scores) # here we have array of scores for each fold, [[], [], []]\n",
    "\tprint('>%s %.3f (%.3f)' % (s, mean(scores), std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "print(results)\n",
    "print(strategies)\n",
    "pyplot.boxplot(results, labels=strategies, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9435363c",
   "metadata": {},
   "source": [
    "##### Interpreating the above chart\n",
    "- Every kfold validation has split 10 and repeate 3, So it will have 30 experiments. \n",
    "- Each 30 time we get a accurecy score.  that 30 scores has been plotted as one box.\n",
    "- Green triangle is the mean of the accurecy of all the 30 experiments.\n",
    "- Orange line is median of the accurecy for all the 30 experiments.\n",
    "- Box spread is acurecy range for all 30 experiments. Box height is IQR.\n",
    "- Each startegy has one box.\n",
    "    - mean ~ median ((green triangle near orange line)) means:  The distributions are relatively symmetric, with no extreme outliers pulling the average away from the median.\n",
    "    - mean is higher: that startegy may be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411aa858",
   "metadata": {},
   "source": [
    "##### Predicting using simpleimputer.\n",
    "\n",
    "- We used pipeline to use SimpleImputer, it will only apply the imputer on train data and then separetely apply Imputer on test data during kfoldvalidation.\n",
    "- So when we are predcting using the model, we should use the same pipeline for record for which we are predcting.\n",
    "\n",
    "- **How Imputation Works in Pipelines**\n",
    "    - When you call pipeline.fit(X, y), the imputer learns the imputation values from the training data and stores them. During prediction, it uses these stored values, not values calculated from the new data.\n",
    "    - **For Constant Strategy**: by default it uses value `0`.\n",
    "        - We can change the value using `SimpleImputer(strategy=\"constant\", fill_value=999)`\n",
    "        - What constant or value it learns/stores during tarining, it will use same value for prediction.\n",
    "    - **For Mean Strategy**: \n",
    "        - During training (fit): The imputer calculates and stores the mean of each column from the training data (X)\n",
    "        - During prediction (transform): The imputer uses the stored means to impute missing values in the new data (X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e406f06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2\n"
     ]
    }
   ],
   "source": [
    "# constant imputation strategy and prediction for the horse colic dataset\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv('./data/horse-colic.csv', header=None, na_values='?')\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# create the modeling pipeline\n",
    "pipeline = Pipeline(steps=[('i', SimpleImputer(strategy='constant')), ('m', RandomForestClassifier())])\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# define new data\n",
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, nan, 2, 5, 4, 4, nan, nan, nan, 3, 5, 45.00, 8.40, nan, nan, 2, 11300, 00000, 00000, 2]\n",
    "\n",
    "# make a prediction\n",
    "yhat = pipeline.predict([row]) # use the same pipeline to predict, So that it will handle missing value in the same way as it has imputed during training.\n",
    "\n",
    "# summarize prediction\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6ac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored statistics: [4.         4.66666667]\n",
      "Imputed data: [[ 4.         10.        ]\n",
      " [ 9.          4.66666667]]\n"
     ]
    }
   ],
   "source": [
    "# Demo for How imputer works internally\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Training data\n",
    "X_train = np.array([[1, 2], [3, 4], [5, np.nan], [7, 8]])\n",
    "\n",
    "# Create and fit imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(X_train)\n",
    "\n",
    "# The imputer stored the mean of each column\n",
    "print(\"Stored statistics:\", imputer.statistics_)  \n",
    "# Output: [4.  4.66666667]  <- mean of col 0: (1+3+5+7)/4, mean of col 1: (2+4+8)/3\n",
    "\n",
    "# New data with missing values\n",
    "X_new = np.array([[np.nan, 10], [9, np.nan]])\n",
    "\n",
    "# Transform uses the STORED means, not means from X_new\n",
    "X_imputed = imputer.transform(X_new)\n",
    "print(\"Imputed data:\", X_imputed)\n",
    "# Output: [[4.         10.        ]   <- used stored mean 4.0 for missing value\n",
    "#          [9.          4.66666667]]  <- used stored mean 4.67 for missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b20173",
   "metadata": {},
   "source": [
    "# KNN imputation\n",
    "- A popular approach to missing data imputation is to use a model to predict the missing values.\n",
    "- This requires a model to be created for each input variable that has missing values. \n",
    "- Although any one among a range of different models can be used to predict the missing values.\n",
    "- Missing values must be marked with NaN values and can be replaced with nearest neighbor estimated values.\n",
    "- KNNImputer has 3 main parameters:\n",
    "    - `n_neighbors`: Number of neighbors to use for imputation. The number of neighbors is set to five by default and can be configured by the n neighbors argument\n",
    "    - `metric`: Metric to use for distance calculation. it can be `nan_euclidean`, `manhattan`, `minkowski` etc.\n",
    "    - `weights`: Weights to use for distance calculation. it can be `uniform`, `distance` etc.\n",
    "\n",
    "- **What \"NaN-aware Euclidean distance\" Means**\n",
    "    - when calculating distances, KNNImputer ignores features (columns) that have NaN values in either of the two samples being compared.\n",
    "    - **Regular Euclidean Distance (Not NaN-aware)**\n",
    "        - For two samples with features [x₁, x₂, x₃] and [y₁, y₂, y₃]:\n",
    "            - distance = sqrt((x₁ - y₁)² + (x₂ - y₂)² + (x₃ - y₃)²) \n",
    "            - **Problem**: If any value is NaN, the entire calculation fails or gives NaN.\n",
    "    - **NaN-aware Euclidean Distance**\n",
    "        - Only uses features that are present (not NaN) in BOTH samples:\n",
    "            - Sample A: [1.0, NaN, 3.0, NaN]\n",
    "            - Sample B: [2.0, 5.0, NaN, 4.2]\n",
    "            - `Features 1, 2, and 3 have NaN in at least one sample\n",
    "            - Only feature 0 is available in BOTH\n",
    "\n",
    "            - distance = √[(1.0-2.0)²] = 1.0\n",
    "\n",
    "- **The metric Argument in KNNImputer**\n",
    "    - `KNNImputer(n_neighbors=5, metric='nan_euclidean')  # default`\n",
    "    - Other metrics like `manhattan`, `minkowski` can also be NaN-aware\n",
    "\n",
    "- **The weights Argument in KNNImputer**\n",
    "    - 1. **weights='uniform' (default)**\n",
    "        - All neighbors contribute equally, regardless of their distance.\n",
    "        - \n",
    "    ```python\n",
    "        from sklearn.impute import KNNImputer\n",
    "        import numpy as np\n",
    "\n",
    "        X = np.array([\n",
    "            [1.0, 2.0, 3.0],   # Sample 0\n",
    "            [2.0, 4.0, 5.0],   # Sample 1\n",
    "            [3.0, NaN, 7.0],   # Sample 2 - needs imputation\n",
    "            [10.0, 8.0, 9.0],  # Sample 3\n",
    "        ])\n",
    "\n",
    "        imputer = KNNImputer(n_neighbors=2, weights='uniform')\n",
    "        X_imputed = imputer.fit_transform(X)\n",
    "    ```\n",
    "    - **To impute Sample 2, feature 1**: \n",
    "        - 1. Find 2 nearest neighbors (let's say Samples 0 and 1)\n",
    "        - 2. Simple average of their values: Imputed value = (2.0 + 4.0) / 2 = 3.0\n",
    "\n",
    "    - 2. **weights='distance'**\n",
    "        - **Closer neighbors have MORE influence** than farther neighbors. The weight is inversely proportional to distance.\n",
    "        - To impute Sample 2, feature 1: \n",
    "            - 1. Find 2 nearest neighbors with their distances:\n",
    "                - Sample 0: distance = 2.0, value = 2.0\n",
    "                - Sample 1: distance = 4.0, value = 4.0\n",
    "            - 2. Calculate weights:\n",
    "                - Sample 0: weight_0 = 1.0 / 2.0 = 0.5\n",
    "                - Sample 1: weight = 1.0 / 4.0 = 0.25\n",
    "            - 3. Weighted average: 0.5 + 0.25 = 0.75    \n",
    "            - 4. Calculate imputed value:\n",
    "                - Imputed value = ((2.0 * 0.5) + (4.0 * 0.25)) / 0.75 = (1.0 + 1.0) / 0.75 = 2.67\n",
    "                \n",
    "            - **Sample 0 (closer) contributes **66.7%** (0.5/0.75), while Sample 1 (farther) contributes only **33.3%** (0.25/0.75).**    \n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a99b852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 1605\n",
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "# knn imputation transform for the horse colic dataset\n",
    "from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv('./data/horse-colic.csv', header=None, na_values='?')\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# summarize total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "\n",
    "# define imputer\n",
    "imputer = KNNImputer()\n",
    "\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X)\n",
    "\n",
    "# summarize total missing\n",
    "print('Missing: %d' % sum(isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146eff6b",
   "metadata": {},
   "source": [
    "**Simillary for KnnImputer we need to use Pipeline to avoid data leakage during model training .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d40c4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 28)\n",
      "Mean Accuracy: 0.859 (0.056)\n"
     ]
    }
   ],
   "source": [
    "# evaluate knn imputation and random forest for the horse colic dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv('./data/horse-colic.csv', header=None, na_values='?')\n",
    "print(dataframe.shape)\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# define modeling pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = KNNImputer()\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b12e8a9",
   "metadata": {},
   "source": [
    "**Does number of Neighbour affects KnnImputer behaviour which inturn affects models behaviour ?**\n",
    "\n",
    "- Simillarly we can experiment with weights parameter and metric parameter then see the effect on models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc83a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 0.861 (0.052)\n",
      ">3 0.863 (0.060)\n",
      ">5 0.861 (0.057)\n",
      ">7 0.861 (0.054)\n",
      ">9 0.867 (0.052)\n",
      ">15 0.860 (0.050)\n",
      ">18 0.859 (0.054)\n",
      ">21 0.861 (0.052)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare knn imputation strategies for the horse colic dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv('./data/horse-colic.csv', header=None, na_values='?')\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# evaluate each strategy on the dataset\n",
    "results = list()\n",
    "strategies = [str(i) for i in [1,3,5,7,9,15,18,21]]\n",
    "for s in strategies:\n",
    "\n",
    "\t# create the modeling pipeline\n",
    "\tpipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=int(s))), ('m', RandomForestClassifier())])\n",
    "\n",
    "\t# evaluate the model\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\tscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "\t# store results\n",
    "\tresults.append(scores)\n",
    "\tprint('>%s %.3f (%.3f)' % (s, mean(scores), std(scores)))\n",
    "\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=strategies, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a2ee64",
   "metadata": {},
   "source": [
    "#### Predict using the model trained by data imouted by Knnimputer\n",
    "\n",
    "- Like SimpleImputer, here also we need to use same pipeline to predict from the model, So that the pipeline can use same KnnImputer to impute the missing values in the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn imputation strategy and prediction for the horse colic dataset\n",
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv('./data/horse-colic.csv', header=None, na_values='?')\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# create the modeling pipeline\n",
    "pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=21)), ('m', RandomForestClassifier())])\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# define new data\n",
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, nan, 2, 5, 4, 4, nan, nan, nan, 3, 5, 45.00, 8.40, nan, nan, 2, 11300, 00000, 00000, 2]\n",
    "\n",
    "# make a prediction\n",
    "yhat = pipeline.predict([row])\n",
    "\n",
    "# summarize prediction\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
