{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7035abd",
   "metadata": {},
   "source": [
    "### Decision tree\n",
    "- Decision trees follow an intuitive process to make predictions—one that very much resembles human reasoning.\n",
    "\n",
    "-  ![](./figures/basic_decision_tree.jpeg)\n",
    "\n",
    "- The nodes that have two branches emanating from them are called **decision nodes**, and the nodes with no branches emanating from them are called leaf nodes, or leaves.\n",
    "\n",
    "- The simplest possible decision tree, called a **decision stump**, is formed by a single decision node (the root node) and two leaves. This represents a single yes-or-no question\n",
    "\n",
    "- ![](./figures/decision_tree_1.jpeg)\n",
    "\n",
    "- **How to build this tree ?**\n",
    "- Computers don’t have experience per se, but they have something similar, which is data. Here we will have records having lot of features.\n",
    "- If we wanted to think like a computer, we could just go over all possible trees, try each one of them for some time—say, one year—and compare how well they did by counting how many times we made the right decision using each tree.\n",
    "\n",
    "- Unfortunately, even for a computer, searching over all the possible trees to find the most effective one would take a really long time.\n",
    "\n",
    "- But luckily, we have algorithms that make this search much faster, and thus, we can use decision trees for many wonderful applications, includ- ing spam detection, sentiment analysis, and medical diagnosis.\n",
    "\n",
    "- Each decision node is a question, and each leaf is a prediction. So if we have a set of Questions then we can build a decision tree based on it.\n",
    "\n",
    "- This works for both **classification and regression**.\n",
    "\n",
    "- But there are few question arises for this.\n",
    "- **How exactly do you decide which is the best possible question to ask?**\n",
    "    - We have several ways to do this. The simplest one is using accuracy, which means: which question helps me be correct more often? -> ( **Gini index** or **entropy**)\n",
    "- **Does the process of always picking the best possible question actually get us to build the best decision tree?**\n",
    "    -  Actually, this process does not guarantee that we get the best possible tree. This is what we call a greedy algorithm.\n",
    "- **Why don’t we instead build all the possible decision trees and pick the best one from there?**\n",
    "    -  The problem is that there are so many possible decision trees that it would take a very long time to try them all.\n",
    "- **Where can we find decision trees in real life?**\n",
    "    - They are used extensively in machine learning, not only because they work very well but also because they give us a lot of information on our data. As this explains our model well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a4b549",
   "metadata": {},
   "source": [
    "#### Example Building a decision tree\n",
    "\n",
    "- We have below data.\n",
    "\n",
    "| Platform | Age    | App |\n",
    "| ---------| -----  | --- |\n",
    "| iphone    | Young | Atom Count |\n",
    "| iphone    | Adult | Check Mate Mate  |\n",
    "| Android   | Adult | Beehive Finder |\n",
    "| iphone    | Adult | Check Mate Mate| \n",
    "| Android   | Young | Atom Count|\n",
    "| Android   | Young | Atom Count   |\n",
    "\n",
    "- Finding the best question to ask.\n",
    " - Either the first question will be  **is iphone** ? or **is Young** ? \n",
    "    - lets take **is iphone** ? this will split the data into two groups.\n",
    "        - ![](./figures/by_platform.jpeg)\n",
    "\n",
    "    - lets take **is Young** ? this also split the data into two groups.\n",
    "        - ![](./figures/by_age.jpeg)\n",
    " \n",
    "- We can see that split by age is better, as it figure out a clean boundary between two groups.\n",
    "- We need a way for computer to figure out the best question to ask.\n",
    "\n",
    "- There are 3 ways computer will figure out the best question to ask.\n",
    "    - **Accuracy**\n",
    "    - **Gini index** or  **Gini impurity index**\n",
    "    - **Entropy**\n",
    "\n",
    "\n",
    "##### Accuracy    How often is our model correct? \n",
    "- **Accuracy** is the fraction of correctly classified data points over the total number of data points.\n",
    "    - **Classifier1**: What platform do you use?\n",
    "        - iphone: the majority iphone user using checkmate, we will recommend checkmate, This classifier is correct 2 out of 3 times.\n",
    "        - Android: the majority android user using atomcount, we will recommend atomcount, This classifier is correct 2 out of 3 times.\n",
    "        \n",
    "    - **Classifier2**: What is the age ?\n",
    "        - Young: the majority young user using atomcount, we will recommend atomcount, This classifier is correct 3 out of 3 times.\n",
    "        - Adult: the majority adult user using checkmate, we will recommend checkmate, This classifier is correct 2 out of 3 times.\n",
    "    - Classifier2 is better than Classifier1, as it is correct 5 out of 6 times.   classifier 1 accurecy is 4/6 = 66.6% and classifier 2 accurecy is 5/6 = 83.3%\n",
    "     \n",
    "##### Gini index or Gini impurity index: How diverse is our group?        \n",
    "- In other words, if we have a set in which all the elements are similar, this set has a low Gini index, and if all the elements are different, it has a large Gini index\n",
    "    - **Set 1**: eight red balls, two blue balls\n",
    "    - **Set 2**: four red balls, three blue balls, two yellow balls, one green ball\n",
    "-  If we pick two random elements of the set, what is the probability that they have a different color?   \n",
    "    - `P(picking two balls of different color) = 1 – P(picking two balls of the same color)`\n",
    "    - `P(picking two balls of the same color) = P(both balls are color 1) + P(both balls are color 2) + ... + P(both balls are color n)`\n",
    "\n",
    "    - In general, if pi is the probability that we pick a random ball and it is of color i, then      P(both balls are color i) = pi^2.\n",
    "    - `P(picking two balls of different colors) = 1 – p1^2 – p2^2 – ... – pn^2`.\n",
    "\n",
    "- Set 1: {red, red, red, red, red, red, red, red, blue, blue} (eight red balls, two blue balls)\n",
    "    - Gini index = 1 - (8/10)^2 - (2/10)^2 = 0.32\n",
    "- Set 2: {red, red, red, red, blue, blue, blue, yellow, yellow, green}\n",
    "    - Gini index = 1 - (4/10)^2 - (3/10)^2 - (2/10)^2 - (1/10)^2 = 0.7    \n",
    "\n",
    "- **Applying Gini index to our data**   \n",
    "  - Classifier 1 (by platform):\n",
    "      - Left leaf (iPhone): {A, C, C}\n",
    "      - Right leaf (Android): {A, A, B}\n",
    "  - Classifier 2 (by age):\n",
    "      - Left leaf (young): {A, A, A}\n",
    "      - Right leaf (adult): {B, C, C}\n",
    "  - The Gini indices of the sets {A,C,C},{A,A,B},and{B,C,C}areallthesame: 1−(3/6)^2 −(2/6)^2 = 0.444.\n",
    "  - The Gini index of the set {A,A,A} is 1−(3/3)^2 =0.Ingeneral,the Gini index of a pure set is always 0.  \n",
    "  - Classifier 1's Average Gini index is `(0.444 + 0.444 )/2 = 0.444`.   \n",
    "  - Classifier 2's Average Gini index is `(0 + 0.444 )/2 = 0.222`. \n",
    "  - We conclude that the second split is better, because it has a lower average Gini index.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
