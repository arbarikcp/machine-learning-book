{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c3a6409",
   "metadata": {},
   "source": [
    "#### Understandig Entropy = -Σ(pᵢ × log(pᵢ))\n",
    "\n",
    "- **The Story Behind the Formula**\n",
    "    - This formula was created by Claude Shannon in 1948 when he invented information theory. He asked: \"How do we mathematically measure uncertainty or information?\"\n",
    "\n",
    "- **Shannon's Requirements**\n",
    "    - Shannon said any good measure of uncertainty should satisfy these properties:\n",
    "\n",
    "    - Continuity: Small changes in probabilities → small changes in uncertainty\n",
    "    - Maximum for uniform distribution: Most uncertain when all outcomes equally likely\n",
    "    - Additive for independent events: If you have two independent uncertainties, they should add up\n",
    "    - Monotonicity: More possible outcomes → more uncertainty\n",
    "\n",
    "    - Amazingly, only one formula satisfies all these requirements:**-Σ(pᵢ × log(pᵢ))**\n",
    "\n",
    "    - **Intuitive Derivation (Simplified)**\n",
    "        - Let me build up to the formula step by step:\n",
    "        \n",
    "        - **Step 1: Start with \"Surprise\"**\n",
    "            - When an event with probability p happens, how \"surprised\" should you be?  \n",
    "            - If p = 1 (certain) → No surprise → Surprise = 0\n",
    "            - If p = 0.5 (coin flip) → Some surprise\n",
    "            - If p = 0.01 (rare) → Very surprised\n",
    "\n",
    "        - **Key insight**: Rarer events should give MORE surprise.\n",
    "            - We need a function S(p) where:\n",
    "                - S(1) = 0 (no surprise for certain events)\n",
    "                - S(p) increases as p decreases\n",
    "                - Small p → Large S(p)\n",
    "            - **What function does this?** → S(p) = -log(p) or log(1/p)\n",
    "\n",
    "            - Let's verify:\n",
    "                - S(1) = -log(1) = 0 ✓\n",
    "                - S(0.5) = -log(0.5) = 1 ✓\n",
    "                - S(0.1) = -log(0.1) = 3.32 ✓ (more surprise)\n",
    "                - S(0.01) = -log(0.01) = 6.64 ✓ (even more!) \n",
    "\n",
    "        - **Step 2: Average Surprise = Entropy**\n",
    "            - Now, if you have multiple possible outcomes (like rolling a die), what's the expected surprise on average?\n",
    "            - Expected value formula: **E[X] = Σ(probability × value)**\n",
    "            - So expected surprise (entropy):\n",
    "                - ``` \n",
    "                    Entropy = Σ(pᵢ × Surprise of event i)\n",
    "                          = Σ(pᵢ × (-log(pᵢ)))\n",
    "                          = -Σ(pᵢ × log(pᵢ))  \n",
    "                ```\n",
    "        - Think of it this way:\n",
    "            - Entropy = Average amount of surprise you experience\n",
    "            - Each outcome has:\n",
    "                - Probability pᵢ (how often it happens)\n",
    "                - Surprise -log(pᵢ) (how unexpected it is)\n",
    "\n",
    "\n",
    "        - Multiply them: pᵢ × (-log(pᵢ)) = \"weighted surprise contribution\"\n",
    "        - Sum all contributions: That's your average surprise = Entropy!\n",
    "\n",
    "\n",
    "- #### Why Logarthim\n",
    "    -    The logarithm is there to measure \"surprise\" or \"information content\" in a mathematically meaningful way.\n",
    "\n",
    "- #### Intuitive Explanation\n",
    "    - Think about guessing games:\n",
    "    - Scenario 1: Coin flip (2 outcomes)\n",
    "        - You need to ask 1 yes/no question to figure out the result\n",
    "            \"Is it heads?\"\n",
    "\n",
    "    - Scenario 2: Rolling a 4-sided die (4 outcomes)\n",
    "        - You need 2 yes/no questions to figure out the result\n",
    "        \"Is it 1 or 2?\" → If yes: \"Is it 1?\"\n",
    "        \"Is it 3 or 4?\" → If No: \"Is it 2?\" -> NO then your answer is 1. \n",
    "\n",
    "    - Scenario 3: Rolling an 8-sided die (8 outcomes)\n",
    "        - You need 3 yes/no questions     \n",
    "\n",
    "    - **Notice the pattern?**\n",
    "        - 2 outcomes → 1 question → log₂(2) = 1\n",
    "        - 4 outcomes → 2 questions → log₂(4) = 2\n",
    "        - 8 outcomes → 3 questions → log₂(8) = 3\n",
    "\n",
    "    - **Logarithm converts \"number of possibilities\" into \"number of questions needed\", which is a natural measure of information!**\n",
    "\n",
    "- **The Math Simplified**\n",
    "    - Without Log (doesn't work well):\n",
    "        - If I say \"Impurity\" = p₁ + p₂ + ... -> This always 1 (useless for any impurity measure)\n",
    "        - if I say \"Impurity\" = p₁ × p₂ × ... -> This gets tiny very fast and doesn't capture uncertainty well.\n",
    "\n",
    "    - With Log (works well):\n",
    "        - If I say \"Impurity\" = -Σ(pᵢ × log(pᵢ)) -> This captures uncertainty well and is mathematically meaningful.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a4241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5304930567574826\n",
      "0.0\n",
      "3.1699250014423126\n"
     ]
    }
   ],
   "source": [
    "def entropy_of(data, base=2):\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      - a 1‑D array / list of class labels\n",
    "      - a pandas Series\n",
    "    Returns the Shannon entropy (bits by default).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.stats import entropy\n",
    "\n",
    "    # Convert to a NumPy array (works for list, np.ndarray, pd.Series)\n",
    "    arr = np.asarray(data).ravel()\n",
    "\n",
    "    # Compute frequencies → probabilities\n",
    "    uniq, counts = np.unique(arr, return_counts=True)\n",
    "    probs = counts / counts.sum()\n",
    "\n",
    "    return entropy(probs, base=base)\n",
    "\n",
    "# Usage\n",
    "print(entropy_of([1,1,0,0,0,2,2,2,2])) # medium\n",
    "print(entropy_of([1,1,1,1,1,1])) # low\n",
    "print(entropy_of([1,2,3,4,5,6,7,8,9]))  # high\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6794ba71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy (bits): 1.459147917027245\n",
      "Entropy (df1): 0.0\n",
      "Entropy (df2): 0.9182958340544894\n",
      "Information gain: 0.5408520829727556\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def entropy_from_series(series, base=2):\n",
    "    # value_counts() returns frequencies; normalize=True gives probabilities\n",
    "    probs = series.value_counts(normalize=True).values\n",
    "    return entropy(probs, base=base)\n",
    "\n",
    "# Example\n",
    "df = pd.DataFrame({\n",
    "    \"target\": [\"cat\", \"dog\", \"cat\", \"mouse\", \"dog\", \"cat\"]\n",
    "})\n",
    "print(\"Entropy (bits):\", entropy_from_series(df[\"target\"], base=2))\n",
    "\n",
    "df1 = pd.DataFrame({\n",
    "    \"target\": [\"cat\", \"cat\", \"cat\"]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    \"target\": [\"dog\", \"mouse\", \"dog\"]\n",
    "})\n",
    "print(\"Entropy (df1):\", entropy_from_series(df1[\"target\"], base=2))\n",
    "print(\"Entropy (df2):\", entropy_from_series(df2[\"target\"], base=2))\n",
    "\n",
    "parent_entropy = entropy_from_series(df[\"target\"], base=2)\n",
    "child_entropy = entropy_from_series(df1[\"target\"], base=2) + entropy_from_series(df2[\"target\"], base=2)\n",
    "print(\"Information gain:\", parent_entropy - child_entropy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"target\": [\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\n",
    "     \"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\"]\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
