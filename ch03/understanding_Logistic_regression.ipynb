{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0122adc",
   "metadata": {},
   "source": [
    "### Regression(Linear Regression) Vs Logistic Regression\n",
    "#### Regression(Linear Regression)\n",
    "- **What it does**\n",
    "  - Predicts a continuous numeric value.\n",
    "    - Predicting house prices\n",
    "    - Predicting temperature\n",
    "    - Predicting sales revenue\n",
    "- **Output**\n",
    "  - A real number ‚Äî can be any value from ‚àí‚àû to +‚àû.\n",
    "#### Logistic Regression\n",
    "- **What it does**\n",
    "  - Predicts a probability of a class (classification problem).\n",
    "    - Predict whether an email is spam (1) or not spam (0)\n",
    "    - Predict whether a customer will churn or not churn\n",
    "- **Output**\n",
    "  - A probability (0 to 1) ‚Äî typically converted into class labels (0 or 1).\n",
    "  - ùëù > 0.5 ‚áí Class 1\n",
    "  - ùëù ‚â§ 0.5 ‚áí Class 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c925c",
   "metadata": {},
   "source": [
    "### Understanding Odds\n",
    "\n",
    "- Probability of this outcome is the number of times it occurred divided by the total number of times we ran the experiment.\n",
    "- The odds of this outcome are the number of times it occurred divided by the number of times it didn‚Äôt occur.\n",
    "  - **example 1**:\n",
    "    - the probability of obtaining 1 when we roll a die is 1/6\n",
    "    - but the odds are 1/5\n",
    "  - **example 2**:\n",
    "    - If a particular horse wins 3 out of every 4 races, then the probability of that horse winning a race is 3/4\n",
    "    - And the odds are 3/1 = 3\n",
    "\n",
    "- **Formula**: The formula for odds is simple: if the probability of an event is x, then the odds are (x / 1 ‚àí x) .\n",
    "  - Dice example, odd is. = ( `(1 /6)` / `1 - (1/6)`)  = 1 / 5\n",
    "  - horse example, odd is  = ( `3/4` / `1 - (3/4)`) = `3/4` / `1/4` = 3\n",
    "\n",
    "**Note:** \n",
    "  - the probability is a number between **0 and 1**\n",
    "  - then the odds are a number between **0 and INF**      \n",
    "\n",
    "- Imagine you're looking at 100 emails:\n",
    "  - 80 are spam (class 1)\n",
    "  - 20 are not spam (class 0)\n",
    "- **Probability of spam**:\n",
    "- ```\n",
    "    p = number of spam emails / total emails\n",
    "    p = 80 / 100 = 0.8 (or 80%)\n",
    "  ```\n",
    "- **Simple meaning**: \"If I pick a random email, there's an 80% chance it's spam.\"\n",
    "\n",
    "#### Now, What Are Odds?\n",
    "- **Odds answer a different question**: \"How many times more likely is it to be spam compared to not spam?\"\n",
    "  ```\n",
    "  Odds = (number of spam) / (number of not spam)\n",
    "  Odds = 80 / 20 = 4\n",
    "\n",
    "  Or using probability:\n",
    "  Odds = p / (1 - p)\n",
    "  Odds = 0.8 / 0.2 = 4\n",
    "  ```\n",
    "- **Interpretation**: \"For every 1 legitimate email, there are 4 spam emails.\" \n",
    "- Or: \"Spam is **4 times more likely** than legitimate email.\" \n",
    "\n",
    "##### Example 1: Mostly Spam \n",
    "  ```\n",
    "  Out of 100 emails:\n",
    "  - 90 spam\n",
    "  - 10 legitimate\n",
    "\n",
    "  Probability: p = 90/100 = 0.9 (90% spam)\n",
    "  Odds: 90/10 = 9\n",
    "\n",
    "  Meaning: \"For every 1 legitimate email, there are 9 spam emails\"\n",
    "          \"Spam is 9√ó more likely\"\n",
    "  ``` \n",
    "\n",
    "##### Example 2: Balanced (well-filtered inbox)\n",
    "  ```\n",
    "  Out of 100 emails:\n",
    "  - 50 spam\n",
    "  - 50 legitimate\n",
    "\n",
    "  Probability: p = 50/100 = 0.5 (50% spam)\n",
    "  Odds: 50/50 = 1\n",
    "\n",
    "  Meaning: \"For every 1 legitimate email, there's 1 spam email\"\n",
    "          \"Equal chance\" or \"1:1 odds\"\n",
    "  ```         \n",
    "##### Example 3: Mostly Legitimate (professional inbox)\n",
    "  ```\n",
    "  Out of 100 emails:\n",
    "  - 20 spam\n",
    "  - 80 legitimate\n",
    "\n",
    "  Probability: p = 20/100 = 0.2 (20% spam)\n",
    "  Odds: 20/80 = 0.25\n",
    "\n",
    "  Meaning: \"For every 4 legitimate emails, there's 1 spam email\"\n",
    "          \"Spam is 0.25√ó as likely (or 4√ó less likely)\"\n",
    "  ```\n",
    "\n",
    "##### Example 4: Almost All Spam\n",
    "  ```\n",
    "    Out of 100 emails:\n",
    "    - 99 spam\n",
    "    - 1 legitimate\n",
    "\n",
    "    Probability: p = 99/100 = 0.99 (99% spam)\n",
    "    Odds: 99/1 = 99\n",
    "\n",
    "    Meaning: \"For every 1 legitimate email, there are 99 spam emails\"\n",
    "            \"Spam is 99√ó more likely\"\n",
    "  ```  \n",
    "\n",
    "| Scenario        | Spam  | Legit | Probability(p)| Odds | What it means        |\n",
    "| :---            | :----:| ---:  |---:           | :----|:----                 |\n",
    "| Terrible inbox  | 99    |  1    |0.99           |99    |Spam 99x more likely  |\n",
    "| Bad inbox       | 90    |  10   |0.90           |9     |Spam 9x more likely   |\n",
    "| unfiltered      | 80    |  20   |0.80           |4     |Spam 4x more likely   |\n",
    "| filtered        | 70    |  30   |0.70           |2.3   |Spam 2.3x more likely |\n",
    "| Balanced        | 50    |  50   |0.50           |1     |Equalchance           |\n",
    "| good            | 30    |  70   |0.30           |0.43  |Legit 2.3x more likely|\n",
    "| great           | 20    |  80   |0.20           |0.25  |Legit 4x more likely  |\n",
    "| Excellent       | 10    |  90   |0.10           |0.11  |Legit 9x more likely  |\n",
    "| Perfect         | 1     |  99   |0.01           |0.01  |Legit 99x more likely |\n",
    "\n",
    "\n",
    "#### Key Differences Between Probability and Odds:\n",
    "- **Probability asks**: \"What fraction are spam?\"\n",
    "- **Odds asks**: \"What's the ratio of spam to not spam?\"\n",
    "- **Real Conversational Usage:**\n",
    "  - **Using Probability**:\n",
    "    - \"There's a 90% chance this email is spam\"\n",
    "    - \"I'm 80% sure it's spam\"\n",
    "  - **Using Odds:**\n",
    "    - \"The odds are 9 to 1 it's spam\"\n",
    "    - \"It's 9 times more likely to be spam than legitimate\"\n",
    "    - \"Spam is favored 4:1\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d55fc",
   "metadata": {},
   "source": [
    "#### Why Odds in this Logistic regression\n",
    "- Regression produces a continuous numbers.\n",
    "- But we need Probability which is  (0-1) limited range.\n",
    "- **Odds** generated from the same probablity are range between, 0 to +‚àû  (still not symmetric though)\n",
    "\n",
    "#### Logit  \n",
    "- Take the **logarithm** of odds:\n",
    "- `logit(p) = log(p / (1-p))`\n",
    "- **This is the magical transformation**\n",
    "  \n",
    "| Probability p | Odds | Log-odds (logit) |\n",
    "|---------------|------|------------------|\n",
    "| 0.001 | 0.001 | -6.9 |\n",
    "| 0.1 | 0.11 | -2.2 |\n",
    "| 0.5 | 1 | 0 |\n",
    "| 0.9 | 9 | 2.2 |\n",
    "| 0.99 | 99 | 4.6 |\n",
    "| 0.999 | 999 | 6.9 |\n",
    "\n",
    "- **Why log-odds?**\n",
    "- Now after taking the log of the odds the range has been transformed to -‚àû  to  +‚àû ((matches linear regression model range!))\n",
    "- Symmetric around 0\n",
    "- p = 0.5 ‚Üí log-odds = 0 (neutral point)\n",
    "\n",
    "#### Model\n",
    "- Now we can write:\n",
    "  - **log-odds** = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + b\n",
    "         = w·µÄx + b\n",
    "         = z  (we call this \"z\" or \"net input\")\n",
    "\n",
    "##### Intutive meaning\n",
    "- ```\n",
    "    # Example: Predicting spam email\n",
    "    z = w‚ÇÅ(num_exclamation_marks) + w‚ÇÇ(has_word_free) + w‚ÇÉ(sender_unknown) + b\n",
    "\n",
    "    # If z = 2.5 (positive and large):\n",
    "    #   ‚Üí High log-odds \n",
    "    #   ‚Üí High odds\n",
    "    #   ‚Üí High probability of spam\n",
    "\n",
    "    # If z = -3.0 (negative):\n",
    "    #   ‚Üí Low log-odds\n",
    "    #   ‚Üí Low odds  \n",
    "    #   ‚Üí Low probability of spam\n",
    "\n",
    "    ```   \n",
    "\n",
    "#### We need to get probability back from the log-odd\n",
    "- We need to **reverse** the transformations to get back to probability:\n",
    "- **z (net input) ‚Üí logit ‚Üí odds ‚Üí probability**\n",
    "- The **inverse of logit** is the **sigmoid function**:\n",
    "- œÉ(z) = 1 / (1 + e^(-z))\n",
    "- This is the famous **S-shaped curve**:\n",
    "\n",
    "| z (net input) | œÉ(z) (probability) | Interpretation |\n",
    "|---------------|-------------------|----------------|\n",
    "| -10 | 0.00005 | Almost certainly class 0 |\n",
    "| -2 | 0.12 | Likely class 0 |\n",
    "| 0 | 0.5 | Equally likely |\n",
    "| 2 | 0.88 | Likely class 1 |\n",
    "| 10 | 0.99995 | Almost certainly class 1 |\n",
    "\n",
    "\n",
    "#### Complete picture\n",
    "\n",
    "    \n",
    "- **FORWARD (making predictions)**:\n",
    "    \n",
    "    ```\n",
    "    Features (x‚ÇÅ, x‚ÇÇ, ...) \n",
    "        ‚Üì\n",
    "        √ó weights (w‚ÇÅ, w‚ÇÇ, ...)\n",
    "        ‚Üì\n",
    "    z = w·µÄx + b  (net input, range: -‚àû to +‚àû)\n",
    "        ‚Üì\n",
    "    œÉ(z) = 1/(1 + e^(-z))  (sigmoid function)\n",
    "        ‚Üì\n",
    "    p = probability  (range: 0 to 1)\n",
    "\n",
    "    Predicted class=\n",
    "    {\n",
    "\t    1  ‚Äãif p ‚Äã‚â•0.5\n",
    "\t    0  if pi <0.5\n",
    "    }\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e216a0",
   "metadata": {},
   "source": [
    "### How model learns in Logistic regression\n",
    "\n",
    "- Logistic regression Predicts\n",
    "  - p = P(Y=1|X) -> probability of Y=1 given X, It means I have certain features, What we want to predict is what is the probability of this feature set belongs to class Y.( What is the probability that X(feature set) will be classified as Y)\n",
    "  - `p = P(Y=1|X) = 1 / ( 1 + e^z) = 1 / ( 1 + e^-(w1x1 + w2x2 + w3x3+....+wnxn + b)) ` -> z is net input from the record.\n",
    "  - But‚Ä¶ how do we find the best w‚Äôs so that the model‚Äôs predicted probabilities are as close as possible to the true outcomes (0 or 1)?\n",
    "  - That‚Äôs where **likelihood** and **log-likelihood** come in.\n",
    "\n",
    "#### Likelihood\n",
    "- Think of likelihood as a score that tells us how good our model‚Äôs predictions are for the observed data.\n",
    "  - If the model gives high probabilities to the correct class for every data point, the likelihood is high.\n",
    "  - If it gives wrong probabilities, the likelihood is low.\n",
    "- **Likelihood = ‚ÄúHow likely is it that the model with these w values generated our observed data?‚Äù**\n",
    "- **We want to maximize this likelihood** ‚Üí ‚Äúfind w that make the data most likely.‚Äù\n",
    "##### Mathematically\n",
    "- Each data point i has \n",
    "  - yi = actual class (0 or 1)\n",
    "  - pi = predicted probability that Yi = 1\n",
    "- Now:\n",
    "  - if yi = 1, We want pi to be high -> so that it predicts correctly.\n",
    "  - if yi = 0, we want 1-pi to be high -> So that it predicts correctly\n",
    "\n",
    "- **Putting this 2 equation**\n",
    "  -  Li = (pi^yi)(1-pi)^(1-yi)   -> Li is the likelyhood for one sample.\n",
    "  -  L =  (pi^yi)(1-pi)^(1-yi)  -> For all samples\n",
    "- **Undersatnding likelyhood function**\n",
    "  -   Li = Œ† (pi^yi)(1-pi)^(1-yi)\n",
    "  -  Each data point has:\n",
    "     -  yi: the actual label ‚Äî either 1 (positive class) or 0 (negative class)\n",
    "     -  pi: the model‚Äôs predicted probability that yi = 1\n",
    "     -  Now, we want to measure:\n",
    "        - How likely is it that our model‚Äôs predicted probability pi matches what really happened (the observed yi) ?\n",
    "- **‚úÖ Case 1**: when yi =1 (actual class is 1)\n",
    "  - Li = (pi^yi)(1-pi)^(1-yi) = (pi) ((1-pi)^0) = pi -> likelyhood of ith sample is pi\n",
    "  - So if the true label is 1, the likelihood is the **predicted probability of being 1**.\n",
    "  - if pi is high (e.g., 0.9), great ‚Üí high likelihood.\n",
    "  - if pi is low (e.g., 0.1), poor ‚Üí low likelihood.\n",
    "- **üö´ Case 2**: When yi = 0 (actuall class is 0)\n",
    "  - Li = (pi^yi)(1-pi)^(1-yi) = (pi^0) ((1-pi)^1) = 1-pi \n",
    "  - So if the true label is 0, the likelihood is the predicted probability of being 0 is 1-pi\n",
    "  - if pi (probability of 1) is small (e.g 0.1), 1-pi = 0.9 -> good \n",
    "  - if pi (probability of 1) is large (e.g 0.9), 1-pi = 0.1 -> bad \n",
    "- **So this formula elegantly unifies both cases**\n",
    "\n",
    "#### Example:\n",
    "\n",
    "- **Model1**\n",
    "\n",
    "    | i | Actual ùë¶ùëñ| Model predicted ùëùùëñ=ùëÉ(ùë¶=1‚à£ùë•ùëñ)| ùêøi=ùëùùëñ^ùë¶ùëñ(1‚àíùëùùëñ)^(1‚àíùë¶ùëñ)|\n",
    "    |---|----------------|----------------------------------------|----------------------------------------|\n",
    "    | 1 | 1 | 0.9 | 0.9^1 ‚àó(1‚àí0.9)^0 = 0.9 ‚úÖ |\n",
    "    | 2 | 0 | 0.2 | 0.2^1 ‚àó(1‚àí0.2)^1 = 0.8 ‚úÖ |\n",
    "    | 3 | 1 | 0.3 | 0.3^1 ‚àó(1‚àí0.3)^0 = 0.3 ‚ùå|\n",
    "\n",
    "    - **For sample 1**: Actual 1 model predicated 0.9 probability that it is 1, So likilehood of models prediction is correct 0.9 -> high\n",
    "    - **For sample 2**: Actual 0, model predicted 0.2 probability that it is 1, So it means likilehood of models prediction is 0.8 -> high\n",
    "    - **For sample 3**: Actual 1, model predicted 0.3 probability that it is 1, So it means likilehood of models prediction is 0.3, which is low, it means model predicted wrong class.\n",
    "    \n",
    "    - Now, the total likelihood for the whole dataset is: L = l1 * l2 * l3 = 0.9 * 0.8 *0.3 = 0.216 -> Thats the probability that model generated correct label for all 3 data points.\n",
    "- **Model2**\n",
    "  - If we change the weights(w's) and get better prediction , suppose say p = [ 0.95, 0.05, 0.8], \n",
    "  - Li = [0.95,0.95,0.8]\n",
    "  - Then the probability that model generated correct label for all the samples are, L = 0.95 * 0.95 * 0.8 = 0.722 -> which is much higher.   \n",
    "\n",
    "- Pi ^ yi -> means probability of model gave to the true class 1\n",
    "- (1-pi) ^ (1-yi) -> means probability of model gave to the true class 0\n",
    "- L -> Combined likelihood for this data point being predicted correctly\n",
    "\n",
    "#### Log-likelihood\n",
    "- Products of many probabilities can become very small numbers (numerical instability).\n",
    "- So we take the logarithm ‚Äî it turns the product into a sum (much easier to handle).\n",
    "- log L = ‚àë[ log ((Pi ^ yi)* ((1-pi) ^ (1-yi))) ]\n",
    "  - = ‚àë [ log(Pi ^ yi) + log((1-pi) ^ (1-yi)) ]\n",
    "  - = ‚àë [ yi*log(pi) + (1-yi)*log(1-pi) ]\n",
    "- **This is called the log-likelihood.**\n",
    "- Our goal is to maximize it -> find w's that makes this expression as large as possible. \n",
    "\n",
    "#### negative log likelihood\n",
    "- In optimization, algorithms like gradient descent are usually written to minimize something (a ‚Äúloss‚Äù).\n",
    "- So instead of maximizing the log-likelihood, we minimize the negative log-likelihood:\n",
    "- So we define our loss function as Loss = -logL = - ‚àë [ yi*log(pi) + (1-yi)*log(1-pi) ]\n",
    "\n",
    "- This is known as the **Logistic Loss** or **Binary Cross-Entropy Loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298c8dd",
   "metadata": {},
   "source": [
    "### Different approach during prediction and training (intution)\n",
    "- In logistic regression model while prediction\n",
    "  - Predict the probability of a class being 1 , if pi >= 0.5  then it is claas 1 else class 0\n",
    "- During training  this 0/1 doesn't make sense.\n",
    "  - When learning (training), logistic regression never converts probabilities into 0/1.\n",
    "  - Instead, it keeps the full probability ùëùùëñ and tries to make the probability of the true class as high as possible using the likelihood (or equivalently, minimizing log loss):\n",
    "  - This allows the model to gradually learn ‚Äî it knows ‚Äúhow wrong‚Äù it was even if the prediction was 0.4 vs 0.6.\n",
    "- **Why we don‚Äôt just compare 0/1 predictions during learning**\n",
    "  - | Actual (y_i) | Predicted (p_i) | Thresholded prediction | Correct? | Continuous loss  |\n",
    "    | ------------ | --------------- | ---------------------- | -------- | ---------------- |\n",
    "    | 1            | 0.9             | 1                      | ‚úÖ        | Very small loss  |\n",
    "    | 1            | 0.6             | 1                      | ‚úÖ        | Small loss       |\n",
    "    | 1            | 0.51            | 1                      | ‚úÖ        | Still small loss |\n",
    "    | 1            | 0.49            | 0                      | ‚ùå        | Large loss       |\n",
    "    | 1            | 0.1             | 0                      | ‚ùå        | Very large loss  |\n",
    " - If you just use 0/1 classification:\n",
    "   - All ‚úÖ examples get the same score ‚Äî no gradient difference between 0.6 and 0.95.\n",
    "   - You lose information about how confident the model was.\n",
    "   - But the logistic loss keeps that information ‚Äî it gives a smooth gradient:\n",
    "     - 0.95 ‚Üí tiny loss\n",
    "     - 0.6 ‚Üí moderate loss\n",
    "     - 0.1 ‚Üí huge loss\n",
    "    - That‚Äôs why gradient descent works ‚Äî because the loss changes smoothly with\n",
    "- **Intuitive Analogy**\n",
    "  - Imagine training a dart player\n",
    "    - If you only say ‚Äúhit‚Äù or ‚Äúmiss‚Äù (0/1), he gets no idea how close he was.\n",
    "    - But if you tell him how far he missed, he can adjust gradually.\n",
    "    - The ‚Äúhow far‚Äù feedback = **probabilistic loss**.\n",
    "    - The ‚Äúhit/miss‚Äù feedback = **classification after training**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "la",
   "language": "python",
   "name": "la"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
