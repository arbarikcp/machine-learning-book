{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dedc14e",
   "metadata": {},
   "source": [
    "# classification metrics and regression metrics\n",
    "\n",
    "#### The Key Difference\n",
    "- **Classification**: Predicting categories/classes (discrete outcomes)\n",
    "    - Examples: spam/not spam, pass/fail, disease/healthy, cat/dog/bird\n",
    "- **Regression**: Predicting continuous values (numeric outcomes)\n",
    "    - Examples: house price, stock price, temperature, weight\n",
    "\n",
    "### Classification Metrics\n",
    "- Used when your output is a category or label.\n",
    "#### 1. Confusion Matrix-Based Metrics\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: Quality of positive predictions\n",
    "- **Recall (Sensitivity)**: Ability to find all positives\n",
    "- **Specificity**: Ability to find all negatives\n",
    "- **F1 Score**: Balance between precision and recall\n",
    "- **F-beta Score**: Weighted balance favoring precision or recall\n",
    "\n",
    "- **Confusion Matrix**\n",
    "    - **True Positives** (TP): Correctly predicted positive cases -> positive means, what we are predicting is positive. if in our model we are finding if a person has a disease or not, then TP means the person has the disease and our model predicted it correctly \n",
    "    - **True Negatives** (TN): Correctly predicted negative cases\n",
    "    - **False Positives** (FP): Incorrectly predicted positive cases ->  if in our model we are finding if a person has a disease or not, then FP means the person does not have the disease but our model predicted it as positive \n",
    "    - **False Negatives** (FN): Incorrectly predicted negative cases -> if in our model we are finding if a person has a disease or not, then FN means the person has the disease but our model predicted it as negative\n",
    "\n",
    "    - **Note**: the first part True/False is our prediction is right/wrong with respect to the actual value and the second part Pos/Neg is what we predicted.\n",
    "        - True Positives: Actual value is positive and predicted value is positive\n",
    "        - True Negatives: Actual value is negative and predicted value is negative\n",
    "        - False Positives: Actual value is negative and predicted value is positive (also called Type 1 error)\n",
    "        - False Negatives: Actual value is positive and predicted value is negative (also called Type 2 error)\n",
    "\n",
    "    - **Example**: \n",
    "        - Suppose we are predicting if a student passes or fails an exam based on study hours:\n",
    "        - Suppose TP = 80, TN = 60, FP = 10, FN = 20\n",
    "    \n",
    "    - **Accuracy**: Overall correctness\n",
    "        - How many predictions were correct overall?\n",
    "        - Accuracy = `(TP + TN) / (TP + TN + FP + FN)` -> (total number of correct predictions) / (total number of predictions)\n",
    "        - Problem: Misleading with imbalanced data. If 95% students pass, predicting \"everyone passes\" gives 95% accuracy but is useless!\n",
    "        - (80 + 60) / 170 = 0.82 or **82%**\n",
    "\n",
    "    - **Precision**: Quality of positive predictions, how many of the predicted positive cases are actually positive\n",
    "        - Precision = `TP / (TP + FP)` -> (number of true positive predictions) / (total number of positive predictions)\n",
    "        - Use case: Important when false positives are costly (e.g., spam detectionâ€”you don't want real emails marked as spam).\n",
    "        - 80 / (80 + 10) = 0.89 or **89%**\n",
    "\n",
    "    - **Recall:  (Sensitivity, True Positive Rate)**\n",
    "        - From all positive, actually how many we predicted correctly as positive.\n",
    "            - example: Of all students who actually passed, how many did we predict correctly?\n",
    "        - Recall = `TP / (TP + FN)`,   TP: Actual positive, we predicted positive, FN: Actual positive and we predicted negative.\n",
    "        - 80/ (80 + 20) = 0.80 or **80%**\n",
    "\n",
    "    - **Sensitivity**: Same as recall\n",
    "\n",
    "    -  **Specificity (True Negative Rate)** \n",
    "        - Of all students who actually failed, how many did we predict correctly?\n",
    "        - Specificity: `TN / (TN + FP)`\n",
    "        - 60 / (60 + 10) = 0.86 or **86%**\n",
    "\n",
    "    - **Confusion matrix for a covid prediction model**.\n",
    "        - ![](./figures/cf_matrix.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cases where one metric is more important than other\n",
    "\n",
    "#### âœ… 1. Use cases where RECALL is very important (Precision not critical)\n",
    "- These situations care about finding ALL positives, even if we raise some false alarms.\n",
    "- **Recall** or **Sensitivity** means From all positive how many we predicted correctly.  So in situation where we need to predict all positives correctly, So we need to push recall to 100%. \n",
    "-  Recall = `TP / (TP + FN)` -> So have to make FN as close to 0. If we try to make FN to 0 then FP will increase.\n",
    "\n",
    "    - **ðŸ“Œ 1.1 Cancer screening (initial stage)**\n",
    "        - Goal: Catch every possible cancer case.\n",
    "        - Better to flag 100 people and later verify, than miss 1 real patient.\n",
    "        - Missing a real cancer case = critical failure (FN is terrible).\n",
    "        - Extra false alarms (FP) â†’ just more tests â†’ acceptable.\n",
    "        ðŸ‘‰ High Recall, low Precision is okay.\n",
    "\n",
    "    - **ðŸ“Œ 1.2 Fraud Detection (first-level alert)**\n",
    "        -  Banks run models to identify any suspicious transaction.\n",
    "        -  Missing a fraud â†’ loses money\n",
    "        -  But false alarms â†’ manual review â†’ acceptable \n",
    "        - ðŸ‘‰ Catch everything (high recall), even if many alerts are wrong.\n",
    "\n",
    "    - **ðŸ“Œ 1.3 Search-and-rescue drone detection**\n",
    "        - Drones detect humans in disaster zones.\n",
    "        - Missing a survivor â†’ life-threatening\n",
    "        - False positives â†’ just check more locations \n",
    "        - ðŸ‘‰ Maximize recall even if precision drops.  \n",
    "\n",
    "    - **ðŸ“Œ 1.4 Spam detection (very early stage)**\n",
    "        - If an email might be spam, it can be flagged for review.\n",
    "        - Missing real spam â†’ user sees harmful mail\n",
    "        - But marking real mail as â€œmaybe spamâ€ and showing a warning â†’ acceptable\n",
    "        - ðŸ‘‰ Recall >> Precision here.\n",
    "\n",
    "    - **ðŸ”¥ Summary of high-recall problems**\n",
    "        - Missing a positive is very bad\n",
    "        - Extra positives (FP) are tolerable\n",
    "        - Examples: Cancer, fraud alerting, rescue, safety-critical detection    \n",
    "\n",
    "#### âœ… 2. Use cases where PRECISION is very important (Recall not critical)\n",
    "- Here, you care about being right when predicting positive, and you can afford missing some positives.\n",
    "- False positive are not afforable, TP / (TP + FP) , So in this case we try to make FP close to 0. If we try to do FP close to 0, then FN will increase.\n",
    "    - **ðŸ“Œ 2.1 Missile strike on an terrorist hideout or Police / Criminal identification**\n",
    "        - You cannot wrongly hit other target.\n",
    "        - False positives (innocent people getting killed / flagged) â†’ catastrophic\n",
    "        - Missing a terrorist (false negative) â†’ someone else might catch later\n",
    "        - ðŸ‘‰ Precision extremely important.\n",
    "    - **ðŸ“Œ 2.2 Recommendation systems (e.g., Netflix, Amazon)**\n",
    "        - Netflix suggests movies.\n",
    "        - Showing a bad recommendation â†’ poor experience (FP)\n",
    "        - Missing a good movie â†’ okay, it can recommend later\n",
    "        - ðŸ‘‰ Better to show fewer but accurate suggestions â†’ precision.  \n",
    "\n",
    "    - **ðŸ“Œ 2.3 Advertising (Click-through Prediction)**     \n",
    "        - You show an ad only when user is very likely to click.\n",
    "        - Showing irrelevant ads â†’ money wasted\n",
    "        - Not showing an ad to a potential user â†’ minor loss\n",
    "        - ðŸ‘‰ Precision > Recall. \n",
    "\n",
    "    - **ðŸ“Œ 2.4 Job candidate shortlisting**\n",
    "        - You only want strong candidates.\n",
    "        - Selecting weak candidates = huge waste of time\n",
    "        - Missing some good candidates = manageable\n",
    "        - ðŸ‘‰ Precision matters more.   \n",
    "\n",
    "    - **ðŸ”¥ Summary of high-precision problems**\n",
    "        - Being wrong is very costly\n",
    "        - Missing positives is tolerable\n",
    "        - Examples: policing, recommendations, ads, hiring     \n",
    "\n",
    "#### âœ… 3. Use cases where BOTH Precision and Recall are very important\n",
    "- **ðŸ“Œ 3.1 Medical Diagnosis (final confirmation stage)**\n",
    "    - After initial screening, a physician reviews the final ML output.\n",
    "    - Missing disease â†’ dangerous\n",
    "    - False disease prediction â†’ unnecessary treatment / trauma\n",
    "    - ðŸ‘‰ Need both high recall and high precision.   \n",
    "\n",
    "- **ðŸ“Œ 3.2 Autonomous vehicles (pedestrian detection)**\n",
    "    - Car must: Detect ALL pedestrians (recall)\n",
    "    - Not detect trash cans as pedestrians (precision)\n",
    "    - Both errors are dangerous.   \n",
    "\n",
    "- **ðŸ“Œ 3.3 Credit card fraud blocking (automatic block)**\n",
    "    - If system automatically blocks the card:\n",
    "    - Blocked wrongly â†’ bad customer experience\n",
    "    - Missed fraud â†’ money lost\n",
    "    - Both errors expensive â†’ F1-score matters.\n",
    "\n",
    "- **ðŸ“Œ 3.4 Content moderation: deleting harmful content**\n",
    "    - Platform must: Capture all harmful content (recall)\n",
    "    - Not accidentally delete normal posts (precision)\n",
    "    - Both matter for user experience + safety.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use sklearn to calculate metrics (Using Logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae05bd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STUDENT EXAM PASS/FAIL CLASSIFICATION\n",
      "============================================================\n",
      "\n",
      "Dataset Preview:\n",
      "   study_hours  attendance  previous_score  pass_exam\n",
      "0            7          96              51          0\n",
      "1            4          92              59          0\n",
      "2           13          45              67          0\n",
      "3           11          67              80          0\n",
      "4            8          67              83          0\n",
      "5           13          83              37          1\n",
      "6            5          83              56          0\n",
      "7            7          59              56          0\n",
      "8           10          69              50          0\n",
      "9            3          50              59          0\n",
      "\n",
      "Total Students: 200\n",
      "Passed: 55 (27.5%)\n",
      "Failed: 145 (72.5%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataset: Student Exam Pass/Fail Prediction\n",
    "# Features: study_hours, attendance_percentage, previous_score\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 200 students\n",
    "n_samples = 200\n",
    "\n",
    "study_hours = np.random.randint(1, 15, n_samples)\n",
    "attendance = np.random.randint(40, 100, n_samples)\n",
    "previous_score = np.random.randint(30, 100, n_samples)\n",
    "\n",
    "# Create target: Pass (1) or Fail (0)\n",
    "# Students pass if they study enough AND have good attendance OR high previous score\n",
    "pass_probability = (study_hours * 0.3 + attendance * 0.4 + previous_score * 0.3) / 100\n",
    "pass_exam = (pass_probability + np.random.normal(0, 0.15, n_samples)) > 0.6\n",
    "pass_exam = pass_exam.astype(int)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'study_hours': study_hours,\n",
    "    'attendance': attendance,\n",
    "    'previous_score': previous_score,\n",
    "    'pass_exam': pass_exam\n",
    "})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STUDENT EXAM PASS/FAIL CLASSIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nDataset Preview:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nTotal Students: {len(df)}\")\n",
    "print(f\"Passed: {sum(pass_exam)} ({sum(pass_exam)/len(df)*100:.1f}%)\")\n",
    "print(f\"Failed: {len(df) - sum(pass_exam)} ({(len(df)-sum(pass_exam))/len(df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645024dd",
   "metadata": {},
   "source": [
    "#### Craete a logistic regression to predict pass or fail on above data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370d7eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set: 140 students\n",
      "Testing set: 60 students\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target\n",
    "X = df[['study_hours', 'attendance', 'previous_score']]\n",
    "y = df['pass_exam']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train)} students\")\n",
    "print(f\"Testing set: {len(X_test)} students\")\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = logistic_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc45334",
   "metadata": {},
   "source": [
    "#### Calculate all the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b67d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Metrics:\n",
      "Accuracy: 0.72\n",
      "Precision: 0.25\n",
      "Recall: 0.15\n",
      "F1 Score: 0.19\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85649804",
   "metadata": {},
   "source": [
    "#### Display confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a30c2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CONFUSION MATRIX\n",
      "============================================================\n",
      "\n",
      "                Predicted\n",
      "               Fail  Pass\n",
      "Actual Fail  [[  41     6]]\n",
      "       Pass  [[  11     2]]\n"
     ]
    }
   ],
   "source": [
    "# Display Confusion Matrix\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 60)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\n                Predicted\")\n",
    "print(\"               Fail  Pass\")\n",
    "print(f\"Actual Fail  [[{cm[0][0]:4d}  {cm[0][1]:4d}]]\")\n",
    "print(f\"       Pass  [[{cm[1][0]:4d}  {cm[1][1]:4d}]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e804f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Decision Tree model\n",
    "logistic_model = DecisionTreeClassifier(max_depth=3, criterion='gini', random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_decision_tree = logistic_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d4a0680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Metrics:\n",
      "Accuracy: 0.83\n",
      "Precision: 1.00\n",
      "Recall: 0.23\n",
      "F1 Score: 0.38\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_decision_tree)\n",
    "precision = precision_score(y_test, y_pred_decision_tree)\n",
    "recall = recall_score(y_test, y_pred_decision_tree)\n",
    "f1 = f1_score(y_test, y_pred_decision_tree)\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d4ea4",
   "metadata": {},
   "source": [
    "#### F1 Score\n",
    "- If we want a balance between precision and recall, then We need F1 Score\n",
    "- Harmonic mean of Precision and Recallâ€”balances both metrics.\n",
    "- **Formula:** 2 Ã— (Precision Ã— Recall) / (Precision + Recall)\n",
    "- **Use case**: Good for imbalanced datasets when you need balance between precision and recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b23fa",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "\n",
    "- **âœ… 1. Important Foundation**\n",
    "\n",
    "- A classifier (like logistic regression, random forest, etc.) outputs probabilities:\n",
    "    - P(y=1 | x) = 0.87\n",
    "    - P(y=1 | x) = 0.32\n",
    "    - P(y=1 | x) = 0.51\n",
    "- To convert probabilities â†’ class labels, you choose a threshold:\n",
    "    - If p >= threshold â†’ predict 1\n",
    "    - Else â†’ 0\n",
    "- Default threshold = 0.5 \n",
    "\n",
    "- **Changing the threshold changes:**\n",
    "    - precision\n",
    "    - recall\n",
    "    - false positives\n",
    "    - false negatives\n",
    "- **âœ… 2. How ROC Curve is Connected to these metrics**\n",
    "    - ðŸ‘‰ ROC tells you how good the model is across all possible thresholds.   From there we can choose what is best threshold for our use case.\n",
    "\n",
    "- **Receiver-operating characteristic curve (ROC)**:\n",
    "\n",
    "- The ROC curve is a visual representation of model performance across all thresholds. The long version of the name, receiver operating characteristic, is a holdover from WWII radar detection.\n",
    "\n",
    "- The ROC curve is drawn by calculating the true positive rate (TPR) and false positive rate (FPR) at every possible threshold (in practice, at selected intervals), then graphing TPR over FPR.\n",
    "\n",
    "- A perfect model, which at some threshold has a TPR of 1.0 and a FPR of 0.0, it means it true positive rate is 100% and there is no False positive rate.\n",
    "\n",
    "- ![](./figures/ROC.jpeg)\n",
    "\n",
    "- The perfect model above, containing a square with sides of length 1, has an area under the curve (AUC) of 1.0. This means there is a 100% probability that the model will correctly rank a randomly chosen positive example higher than a randomly chosen negative example.\n",
    "\n",
    "- For a binary classifier, a model that does exactly as well as random guesses or coin flips has a ROC that is a diagonal line from (0,0) to (1,1). The AUC is 0.5, representing a 50% probability of correctly ranking a random positive and negative example.\n",
    "\n",
    "- ![](./figures/ROC_1.jpeg)\n",
    "\n",
    "- The points on a ROC curve closest to (0,1) represent a range of the best-performing thresholds for the given model.\n",
    "\n",
    "-  Consider the points A, B, and C in the following diagram, each representing a threshold:\n",
    "\n",
    "- ![](./figures/ROC_2.jpeg)\n",
    "\n",
    "- If false positives (false alarms) are highly costly, it may make sense to choose a threshold that gives a lower FPR, like the one at point A, even if TPR is reduced. \n",
    "- Conversely, if false positives are cheap and false negatives (missed true positives) highly costly, the threshold for point C, which maximizes TPR, may be preferable. \n",
    "- If the costs are roughly equivalent, point B may offer the best balance between TPR and FPR.\n",
    "\n",
    "#### AUC\n",
    "- Area under the ROC curve (AUC) measures overall ranking ability:\n",
    "    - AUC 0.5 â†’ random guessing\n",
    "    - AUC 1.0 â†’ perfect separation\n",
    "    - AUC 0.8 â†’ good model\n",
    "\n",
    "- **ðŸŽ¯ Final Step â€” How do we get the required Precision/Recall?**\n",
    "- 1. Compute Precision & Recall for many thresholds\n",
    "\n",
    "    | Threshold | Precision | Recall |\n",
    "| --------- | --------- | ------ |\n",
    "| 0.9       | 0.95      | 0.20   |\n",
    "| 0.7       | 0.80      | 0.60   |\n",
    "| 0.5       | 0.70      | 0.75   |\n",
    "| 0.3       | 0.55      | 0.90   |\n",
    "| 0.1       | 0.20      | 0.99   |\n",
    "\n",
    "- 2. Choose threshold based on your business requirement\n",
    "    - **Want high Recall?**\n",
    "        - Use lower threshold (0.3â€“0.1)\n",
    "            - â†’ you catch more positives\n",
    "            - â†’ but precision drops\n",
    "\n",
    "    - Good for:\n",
    "        - Cancer detection\n",
    "        - Fraud detection\n",
    "        - Intrusion detection\n",
    "\n",
    "    - **Want high Precision?**\n",
    "        - Use higher threshold (0.7â€“0.9)\n",
    "        - â†’ fewer false positives\n",
    "        - â†’ but recall drops\n",
    "    - Good for:\n",
    "        - Email: predicting \"SPAM\"\n",
    "        - Recommender systems\n",
    "        - Ads click prediction \n",
    "  \n",
    "    - **Want balance?**\n",
    "        - Use: \n",
    "            - F1 score\n",
    "            - Youdenâ€™s index (max TPR âˆ’ FPR)\n",
    "            - Equal-error rate threshold       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3eed58fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, f1_score\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39adcbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1. Generate sample data ----\n",
    "X, y = make_classification(n_samples=2000, n_features=10,\n",
    "                           weights=[0.9, 0.1], random_state=42)  # imbalanced\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# ---- 2. Train model (probability output) ----\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "probs = model.predict_proba(X_test)[:, 1]  # P(y=1|x), the probability estimates for the positive class, not the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d91e127e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test shape (600,)\n",
      "probs shape (600,)\n",
      "Shape of unique probabilities: (600,)\n",
      "fpr shape (54,)\n",
      "tpr shape (54,)\n",
      "AUC: 0.931\n",
      "AUC: 0.9313888279708011\n",
      "Best threshold (Youden J): 0.17109563962724111\n",
      "TPR: 0.847457627118644 FPR: 0.06839186691312385\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# PART A: ROC + Optimal Threshold using Youden's J Statistic\n",
    "# ==============================================================\n",
    "\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, probs)\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "\n",
    "# Get unique values of probabilities\n",
    "unique_probs = np.unique(probs)\n",
    "\n",
    "print(f\"y_test shape {y_test.shape}\")\n",
    "print(f\"probs shape {probs.shape}\")\n",
    "\n",
    "# Print the unique values and their shape\n",
    "#print(f\"Unique probabilities: {unique_probs}\")\n",
    "print(f\"Shape of unique probabilities: {unique_probs.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"fpr shape {fpr.shape}\")\n",
    "print(f\"tpr shape {tpr.shape}\")\n",
    "#print(f\"roc_thresholds shape {roc_thresholds.shape}\")\n",
    "print(f\"AUC: {auc:.3f}\")\n",
    "\n",
    "# Youden's J = TPR - FPR\n",
    "j_scores = tpr - fpr\n",
    "j_best_idx = np.argmax(j_scores)\n",
    "roc_best_threshold = roc_thresholds[j_best_idx]\n",
    "\n",
    "print(\"AUC:\", auc)\n",
    "print(\"Best threshold (Youden J):\", roc_best_threshold)\n",
    "print(\"TPR:\", tpr[j_best_idx], \"FPR:\", fpr[j_best_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c5cae9",
   "metadata": {},
   "source": [
    "#### Precesion recall graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45dc61ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best F1 threshold: 0.5408860893577117\n",
      "Precision: 0.782608695652174\n",
      "Recall: 0.6101694915254238\n",
      "F1: 0.6857142857142858\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# PART B: Precisionâ€“Recall Curve + Max F1 Threshold\n",
    "# ==============================================================\n",
    "\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, probs)\n",
    "\n",
    "# Remove last element for threshold alignment (sklearn quirk)\n",
    "pr_thresholds = pr_thresholds\n",
    "\n",
    "# Compute F1 for each threshold\n",
    "f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "best_f1_threshold = pr_thresholds[best_f1_idx]\n",
    "\n",
    "print(\"\\nBest F1 threshold:\", best_f1_threshold)\n",
    "print(\"Precision:\", precision[best_f1_idx])\n",
    "print(\"Recall:\", recall[best_f1_idx])\n",
    "print(\"F1:\", f1_scores[best_f1_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c19453fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold for ~80% precision: 0.7689339741277018 Precision: 0.8 Recall: 0.4067796610169492\n",
      "Threshold for ~80% recall: 0.18810002505368467 Precision: 0.5662650602409639 Recall: 0.7966101694915254\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# PART C: Threshold for Target Precision or Recall\n",
    "# ==============================================================\n",
    "\n",
    "target_precision = 0.80\n",
    "target_recall = 0.80\n",
    "\n",
    "# Nearest threshold achieving target precision\n",
    "idx_prec = np.argmin(np.abs(precision[:-1] - target_precision))\n",
    "print(\"\\nThreshold for ~80% precision:\", pr_thresholds[idx_prec],\n",
    "      \"Precision:\", precision[idx_prec], \"Recall:\", recall[idx_prec])\n",
    "\n",
    "# Nearest threshold achieving target recall\n",
    "idx_rec = np.argmin(np.abs(recall[:-1] - target_recall))\n",
    "print(\"Threshold for ~80% recall:\", pr_thresholds[idx_rec],\n",
    "      \"Precision:\", precision[idx_rec], \"Recall:\", recall[idx_rec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db847b",
   "metadata": {},
   "source": [
    "### Prediction bias\n",
    "\n",
    "- Prediction bias is the difference between the mean of a model's predictions and the mean of ground-truth labels in the data.\n",
    "-  A model trained on a dataset where 5% of the emails are spam should predict, on average, that 5% of the emails it classifies are spam. \n",
    "- In other words, the mean of the labels in the ground-truth dataset is 0.05, and the mean of the model's predictions should also be 0.05. If this is the case, the model has zero prediction bias. But they may have some other issue.\n",
    "- If the model instead predicts 20% of the time that an email is spam, then something is wrong with the training dataset, the new dataset the model is applied to, or with the model itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
