{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb23e3ca",
   "metadata": {},
   "source": [
    "## The core intuition\n",
    "- When a model learns, it tries to fit the training data as best as possible â€” by adjusting weights (Î²â€™s).But if it fits too well, it may start memorizing the data instead of learning general patterns â€” this is called overfitting.\n",
    "- Regularization is a penalty that discourages the model from:\n",
    "  - Making weights too large\n",
    "  - Becoming too complex\n",
    "  - Overreacting to small noise in the data\n",
    "### Think of regularization as \"discipline for the weights\"\n",
    "- Without regularization:\n",
    "  - The model says: â€œIâ€™ll do anything to make my predictions perfect.â€\n",
    "  - It might assign huge coefficients to certain features.\n",
    "- With regularization:\n",
    "  - We say: â€œSure, fit the data â€” but donâ€™t go crazy with those weights.â€\n",
    "  - This keeps the model simpler and more generalizable.\n",
    "### The mathematical idea\n",
    "- We modify the loss function (e.g., logistic loss) by adding a penalty term that depends on the weights.\n",
    "  - We calculate our original loss for all samples. based on this loss ( by taking its partial derivative with respect to weight, we calculate the gradiant descent for that weight)\n",
    "  - With Regularization, lets add a small term to the total loss. \n",
    "    - `New Loss=Original Loss+Î»Ã—Penalty(Weights)`\n",
    "      - ğœ† (lambda) = regularization strength (a tuning hyperparameter)\n",
    "      - Higher ğœ† â†’ more penalty â†’ simpler model\n",
    "      - Lower ğœ† â†’ less penalty â†’ model can fit more freely  \n",
    "### Two common types of regularization\n",
    "#### L2 Regularization (Ridge)\n",
    "- Penalty = sum of squares of the weights:\n",
    "- Penalty= âˆ‘â€‹Î²j^2\n",
    "- So the loss becomes: loss = `âˆ’log-likelihood + Î» âˆ‘â€‹Î²j^2`\n",
    "- **Effect:**\n",
    "  - Penalizes large weights smoothly.\n",
    "  - Tends to shrink weights but not set them exactly to zero.\n",
    "  - Keeps all features, just with smaller influence.\n",
    "  - **Intuition**: â€œDonâ€™t let any single feature dominate â€” share responsibility.â€\n",
    "#### L1 Regularization (Lasso)\n",
    "- Penalty = sum of absolute values of weights:\n",
    "- Penalty=âˆ‘â€‹âˆ£Î²jâ€‹âˆ£\n",
    "- So the loss becomes `Loss=âˆ’log-likelihood+ Î» âˆ‘â€‹âˆ£Î²jâ€‹âˆ£`\n",
    "- **Effect:**\n",
    "  - Encourages sparsity (many weights become exactly 0).\n",
    "  - This means some features get completely dropped â†’ feature selection effect.\n",
    "  - **Intuition**: â€œKeep only the most useful features, ignore the rest.â€\n",
    "\n",
    "#### Elastic Net (Hybrid)\n",
    "- Sometimes, we combine both:\n",
    "  - `Loss= âˆ’log-likelihood+ Î»1â€‹âˆ‘âˆ£Î²jâ€‹âˆ£ + Î»2â€‹âˆ‘Î²j^2â€‹`\n",
    "  - It balances feature selection (L1) and weight shrinkage (L2).\n",
    "\n",
    "### Intuitive analogy\n",
    "- Think of model weights as children in a class:\n",
    "- \n",
    "| Without regularization                     | With L2 regularization                  | With L1 regularization                               |\n",
    "| ------------------------------------------ | --------------------------------------- | ---------------------------------------------------- |\n",
    "| Some kids shout really loud (huge weights) | Everyone speaks softly (weights shrink) | Only the best few kids speak (some weights become 0) |\n",
    "\n",
    "- **Regularization teaches the model to be humble and balanced â€” not overconfident.**\n",
    "\n",
    "### numerical feel\n",
    "- Imagine 2 features, and your model learns Î²â‚ and Î²â‚‚ as weight for these 2 features.\n",
    "-\n",
    "| Weight | Without Reg | L2 Regularized | L1 Regularized |\n",
    "| ------ | ----------- | -------------- | -------------- |\n",
    "| Î²â‚     | 10.0        | 1.8            | 0              |\n",
    "| Î²â‚‚     | -7.0        | -1.2           | -0.9           |\n",
    "\n",
    "- Without reg â†’ large, unstable weights.\n",
    "- L2 â†’ smaller, smoother weights.\n",
    "- L1 â†’ prunes out Î²â‚ completely (set to zero).\n",
    "\n",
    "### When to use which?\n",
    "-\n",
    "| Goal                                                  | Use             |\n",
    "| ----------------------------------------------------- | --------------- |\n",
    "| You want to prevent overfitting but keep all features | **L2 (Ridge)**  |\n",
    "| You want to automatically remove unimportant features | **L1 (Lasso)**  |\n",
    "| You want both stability and sparsity                  | **Elastic Net** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e8b72",
   "metadata": {},
   "source": [
    "### Mathematcally L2 (Ridge) regularization\n",
    "- Normally, in regression or logistic regression, we minimize the loss:\n",
    "  - `J(w)=Loss(all Samples) = âˆ‘â€‹(yiâ€‹âˆ’y^â€‹iâ€‹)^2`\n",
    "- WIth L2 regularization we minimize the Loss + (L2 penalty)\n",
    "  -  `Jregâ€‹(w) = âˆ‘â€‹(yiâ€‹âˆ’y^â€‹iâ€‹)^2 + Î» âˆ‘â€‹wj^2â€‹`\n",
    "- Now we use Optimizer to find the gradiant for each weight.\n",
    "- Letâ€™s compute the derivative of the penalty part:\n",
    "  - `âˆ‚/âˆ‚wj â€‹â€‹(Î»wj^2â€‹)= 2Î»wjâ€‹`\n",
    "- Lets calculate the gradiant of total loss + penalty.\n",
    "  - `wjâ€‹ â† wj â€‹âˆ’Î· (âˆ‚/âˆ‚wj(â€‹Loss)â€‹ + 2Î»wjâ€‹)  `   -> Î· is learning rate, âˆ‚/âˆ‚wj(â€‹Loss) -> gradiant of loss,  2Î»wj -> is the derivative of penalty/regularization part.  thats how we update wj. \n",
    "\n",
    "- That extra **2Î»wj** term is crucial â€” it says:\n",
    "  - **â€œThe bigger wj is, the more Iâ€™ll push it toward zero.â€**\n",
    "- So:\n",
    "  - A big weight (say wj = 10) gets a large correction (pushes it down a lot as 2.Î».wj -> will be some value with respect to wj ), think Î» = 0.1 So 2.Î».wj = 2\n",
    "  - A small weight (say wj = 0.1) gets a tiny correction. -> 2.Î».wj = 0.02 .\n",
    "  - So the first once is penalized by 2, and the second one is penalized by 0.2\n",
    "\n",
    "- **This means large weights are penalized more strongly, automatically.**: Thatâ€™s why the penalty term is not a â€œconstant it depends on the size of each wj\n",
    "\n",
    "#### Why L2 doesnâ€™t push weights exactly to zero\n",
    "- The shrinkage term  2Î»wj is proportional to w_j itself.\n",
    "- That means:\n",
    "  - The bigger wj is, the bigger the shrinkage.\n",
    "  - As wj gets smaller, the shrinkage becomes weaker.\n",
    "  - When wj gets very close to zero, 2Î»wj â‰ˆ 0 â€” the regularization force becomes negligible.\n",
    "  - So the optimizer slows down as it approaches zero and usually stops before hitting exactly 0. â†’ It shrinks continuously, but never sets to 0 exactly.\n",
    "\n",
    "\n",
    "- **Letâ€™s see an example.**\n",
    "\n",
    "- Suppose we are fitting one feature with a weight w. Without regularization, loss is:  J(w)=(yâˆ’wx)^2\n",
    "  - Derivative wrt w: ğ‘‘J/ğ‘‘W = âˆ’2x(yâˆ’wx)\n",
    "\n",
    "- With L2 regularization: ğ½ğ‘Ÿğ‘’ğ‘”(ğ‘¤) = ( ğ‘¦ âˆ’ ğ‘¤ğ‘¥ )^2 + ğœ†ğ‘¤^2\n",
    "  - Derivative wrt w = âˆ’2x(yâˆ’wx)+2Î»w\n",
    "\n",
    "- Now when w is large, that 2Î»w term dominates â†’ strong pull toward zero.\n",
    "- As w gets small, the regularization term weakens â†’ it doesnâ€™t force w to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f064186",
   "metadata": {},
   "source": [
    "### Mathematcally L1 (Lasso) regularization\n",
    "- For L1, the penalty term is: **Î»âˆ‘âˆ£ wj âˆ£**\n",
    "- SO the derivative of the penalty term \n",
    "- ```\n",
    "    âˆ‚/âˆ‚wjâ€‹â€‹(Î». âˆ£wjâ€‹âˆ£ )= +Î»,  â€‹if wjâ€‹>0\n",
    "                     âˆ’Î»,  if wjâ€‹<0\n",
    "                     undefined if wjâ€‹=0â€‹`\n",
    "  ```              \n",
    "\n",
    "- **Calculating loss**\n",
    "  -   `wjâ€‹â† wj â€‹âˆ’ Î· ( âˆ‚/âˆ‚w(Loss) + Î» . sign(wj))`\n",
    "  -   Letâ€™s interpret that:\n",
    "      - If wj >0 â†’ the regularization term subtracts ğœ‚ğœ† , every step â†’ steadily pulls it down toward 0.  \n",
    "      - If wj < 0 â†’ the regularization term adds ğœ‚ğœ† , every step â†’ steadily pulls it up toward 0.  \n",
    "      - If wj = 0 â†’ the sign is undefined, but we can pick 0 if it helps minimize loss.\n",
    "\n",
    "#### The key difference between L2 and L1 regularization\n",
    "- For L2, the â€œpull strengthâ€ = 2Î»wj, which weakens as wjâ†’0. So once a weight is small, the pull becomes almost nothing.\n",
    "- For L1, the â€œpull strengthâ€ = constant Â±Î» â€” even when wj is tiny!,  So the optimizer keeps pushing it toward 0 until it crosses exactly zero â€” where it can stop abruptly.\n",
    "- Thatâ€™s why L1 creates exact zeros â†’ feature selection happens naturally.\n",
    "\n",
    "#### Numeric Intuition between L2 & L1\n",
    "- Letâ€™s say learning rate Î· = 0.1, Î» = 0.5. \n",
    "- Suppose w=1.0\n",
    "- **Start with L2**:\n",
    "  - Update to w,   w = 1.0 âˆ’ 0.1(2Ã—0.5Ã—1.0) = 1.0âˆ’0.1=0.9\n",
    "  - Next update to w:  w= 0.9 âˆ’ 0.1(2Ã—0.5Ã—0.9) = 0.81\n",
    "- **Now L1:**\n",
    "  - Update to w, w = 1.0 âˆ’ 0.1(0.5Ã—1) = 1.0âˆ’0.05 = 0.95\n",
    "  - Next update to w = 0.95 âˆ’ 0.1(0.5Ã—1) = 0.95âˆ’0.05 = 0.90\n",
    "  - Next step: 0.9,0.85,0.8, â€¦ Same-size steps!\n",
    "  - Eventually, it crosses zero and then stops right at zero because the gradient flips sign\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567368f",
   "metadata": {},
   "source": [
    "### Elastic Net (Combined L2 & L1)\n",
    "- Sometimes, you want both effects:\n",
    "  - Shrink weights (for stability)\n",
    "  - Set some to zero (for feature selection)\n",
    "  - Thatâ€™s Elastic Net, combining both penalties:\n",
    "  - `J(w)=Loss+Î»1â€‹ âˆ‘ âˆ£wjâ€‹âˆ£ + Î»2â€‹âˆ‘wj^2â€‹`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f936ab",
   "metadata": {},
   "source": [
    "### Real-world intuition\n",
    "- Letâ€™s say you have 100 features, but only 10 actually matter.\n",
    "\n",
    "- **L2 regularization (Ridge)**\n",
    "    â†’ will make all 100 weights small, including the 90 useless ones (but nonzero).\n",
    "    â†’ Model is stable but not sparse.\n",
    "\n",
    "- **L1 regularization (Lasso)**\n",
    "    â†’ will make many of the 90 irrelevant weights exactly 0.\n",
    "    â†’ Model is sparse, easy to interpret."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "la",
   "language": "python",
   "name": "la"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
